<!DOCTYPE html>
<html lang="zh" dir="ltr">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>stanford</title>
    <meta name="description" content="stanford">
    <meta name="generator" content="VitePress v1.5.0">
    <link rel="preload stylesheet" href="/assets/style.55NQPeqS.css" as="style">
    <link rel="preload stylesheet" href="/vp-icons.css" as="style">
    
    <script type="module" src="/assets/app.7uGsmnbX.js"></script>
    <link rel="modulepreload" href="/assets/chunks/theme.QB-wH4Se.js">
    <link rel="modulepreload" href="/assets/chunks/framework.B1z0IdBH.js">
    <link rel="modulepreload" href="/assets/grok_27.md.B5DKGARG.lean.js">
    <link rel="icon" href="/logo.svg">
    <link rel="icon" type="image/svg+xml" href="/images/logo.png">
    <meta name="theme-color" content="#3c8772">
    <meta property="og:url" content="https://www.252x.com">
    <meta property="og:type" content="website">
    <meta property="og:title" content="stanford">
    <meta property="og:description" content="stanford">
    <meta property="og:image" content="/images/logo.png">
    <meta name="twitter:site" content="@book">
    <meta name="twitter:card" content="summary">
    <link rel="preconnect" href="https://f.543x.com">
    <script>(()=>{const e=(o,r,t=!1)=>{const s=localStorage.getItem(o);(s?s!=="false":t)&&document.documentElement.classList.add(r)};e("vue-docs-prefer-composition","prefer-composition",!0),e("vue-docs-prefer-sfc","prefer-sfc",!0),window.__VUE_BANNER_ID__="vt2024_1",e(`vue-docs-banner-${__VUE_BANNER_ID__}`,"banner-dismissed")})();</script>
    <script>location.search.includes("?uwu")&&document.documentElement.classList.add("uwu");</script>
    <script src="https://cdn.usefathom.com/script.js" data-site="XNOLWPLB" data-spa="auto" defer></script>
    <script src="https://vueschool.io/banner.js?affiliate=book&amp;type=top" async></script>
    <script id="check-dark-mode">(()=>{const e=localStorage.getItem("vitepress-theme-appearance")||"auto",a=window.matchMedia("(prefers-color-scheme: dark)").matches;(!e||e==="auto"?a:e==="dark")&&document.documentElement.classList.add("dark")})();</script>
    <script id="check-mac-os">document.documentElement.classList.toggle("mac",/Mac|iPhone|iPod|iPad/i.test(navigator.platform));</script>
    <link rel="preload" href="/assets/inter-roman-latin.Di8DUHzh.woff2" as="font" type="font/woff2" crossorigin="">
  </head>
  <body>
    <div id="app"><div class="VPApp" data-v-e4982c5a><!--[--><span tabindex="-1" data-v-ebeb79d9></span><a href="#VPContent" class="VPSkipLink visually-hidden" data-v-ebeb79d9>Skip to content</a><!--]--><!----><!--[--><div class="banner" data-v-8f28d446><p class="vt-banner-text" data-v-8f28d446><span class="vt-text-primary" data-v-8f28d446>VueConf Toronto</span><span class="vt-tagline" data-v-8f28d446> - Join the premier TypeScript conference</span><a target="_blank" class="vt-primary-action" href="https://www.543x.com" data-v-8f28d446> Register </a></p><button data-v-8f28d446><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="close" data-v-8f28d446><path d="M18.9,10.9h-6v-6c0-0.6-0.4-1-1-1s-1,0.4-1,1v6h-6c-0.6,0-1,0.4-1,1s0.4,1,1,1h6v6c0,0.6,0.4,1,1,1s1-0.4,1-1v-6h6c0.6,0,1-0.4,1-1S19.5,10.9,18.9,10.9z"></path></svg></button><p class="vt-banner-text vt-coupon" data-v-8f28d446><span class="vt-text-primary" data-v-8f28d446>www</span> 543x <span class="vt-text-primary" data-v-8f28d446>.com</span></p></div><!--]--><header class="VPNav nav-bar stick" data-v-e4982c5a data-v-9cbed0dc><div class="VPNavBar" data-v-9cbed0dc data-v-78ea45ed><div class="container" data-v-78ea45ed><a class="VPNavBarTitle" href="/" data-v-78ea45ed data-v-4b84c549><!--[--><svg class="logo" viewBox="0 0 128 128" width="24" height="24" data-v-4b84c549><path fill="#42b883" d="M78.8,10L64,35.4L49.2,10H0l64,110l64-110C128,10,78.8,10,78.8,10z" data-v-4b84c549></path><path fill="#35495e" d="M78.8,10L64,35.4L49.2,10H25.6L64,76l38.4-66H78.8z" data-v-4b84c549></path></svg><span class="text" data-v-4b84c549>Vue.js</span><!--]--></a><div class="content" data-v-78ea45ed><div class="VPNavBarSearch search" data-v-78ea45ed><!----><div id="docsearch"><button type="button" class="DocSearch DocSearch-Button" aria-label="Search"><span class="DocSearch-Button-Container"><svg width="20" height="20" class="DocSearch-Search-Icon" viewBox="0 0 20 20"><path d="M14.386 14.386l4.0877 4.0877-4.0877-4.0877c-2.9418 2.9419-7.7115 2.9419-10.6533 0-2.9419-2.9418-2.9419-7.7115 0-10.6533 2.9418-2.9419 7.7115-2.9419 10.6533 0 2.9419 2.9418 2.9419 7.7115 0 10.6533z" stroke="currentColor" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path></svg><span class="DocSearch-Button-Placeholder">Search</span></span><!----></button></div></div><nav aria-labelledby="main-nav-aria-label" class="VPNavBarMenu menu" data-v-78ea45ed data-v-2cfd1945><span id="main-nav-aria-label" class="visually-hidden" data-v-2cfd1945>Main Navigation</span><!--[--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">data <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/drive/1.html"><!--[-->data1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/2.html"><!--[-->data2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/3.html"><!--[-->data3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/4.html"><!--[-->data4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/5.html"><!--[-->data5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/6.html"><!--[-->data6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/7.html"><!--[-->data7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/8.html"><!--[-->data8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/9.html"><!--[-->data9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/10.html"><!--[-->data10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/11.html"><!--[-->data11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/12.html"><!--[-->data12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/13.html"><!--[-->data13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/14.html"><!--[-->data14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/15.html"><!--[-->data15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/16.html"><!--[-->data16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/17.html"><!--[-->data17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/18.html"><!--[-->data18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/19.html"><!--[-->data19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/20.html"><!--[-->data20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/21.html"><!--[-->data21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/22.html"><!--[-->data22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/23.html"><!--[-->data23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/24.html"><!--[-->data24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/25.html"><!--[-->data25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/26.html"><!--[-->data26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/27.html"><!--[-->data27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/28.html"><!--[-->data28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/29.html"><!--[-->data29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/30.html"><!--[-->data30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/31.html"><!--[-->data31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/32.html"><!--[-->data32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/33.html"><!--[-->data33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/34.html"><!--[-->data34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/35.html"><!--[-->data35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/36.html"><!--[-->data36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/37.html"><!--[-->data37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/38.html"><!--[-->data38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/39.html"><!--[-->data39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/40.html"><!--[-->data40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/41.html"><!--[-->data41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/42.html"><!--[-->data42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/43.html"><!--[-->data43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/44.html"><!--[-->data44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/45.html"><!--[-->data45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/46.html"><!--[-->data46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/47.html"><!--[-->data47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/48.html"><!--[-->data48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/49.html"><!--[-->data49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/50.html"><!--[-->data50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/51.html"><!--[-->data51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/52.html"><!--[-->data52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/53.html"><!--[-->data53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/54.html"><!--[-->data54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/55.html"><!--[-->data55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/56.html"><!--[-->data56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/57.html"><!--[-->data57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/58.html"><!--[-->data58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/59.html"><!--[-->data59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/60.html"><!--[-->data60<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup active" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">grok <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/grok/1.html"><!--[-->grok1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/2.html"><!--[-->grok2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/3.html"><!--[-->grok3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/4.html"><!--[-->grok4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/5.html"><!--[-->grok5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/6.html"><!--[-->grok6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/7.html"><!--[-->grok7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/8.html"><!--[-->grok8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/9.html"><!--[-->grok9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/10.html"><!--[-->grok10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/11.html"><!--[-->grok11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/12.html"><!--[-->grok12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/13.html"><!--[-->grok13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/14.html"><!--[-->grok14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/15.html"><!--[-->grok15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/16.html"><!--[-->grok16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/17.html"><!--[-->grok17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/18.html"><!--[-->grok18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/19.html"><!--[-->grok19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/20.html"><!--[-->grok20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/21.html"><!--[-->grok21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/22.html"><!--[-->grok22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/23.html"><!--[-->grok23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/24.html"><!--[-->grok24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/25.html"><!--[-->grok25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/26.html"><!--[-->grok26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/27.html"><!--[-->grok27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/28.html"><!--[-->grok28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/29.html"><!--[-->grok29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/30.html"><!--[-->grok30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/31.html"><!--[-->grok31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/32.html"><!--[-->grok32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/33.html"><!--[-->grok33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/34.html"><!--[-->grok34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/35.html"><!--[-->grok35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/36.html"><!--[-->grok36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/37.html"><!--[-->grok37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/38.html"><!--[-->grok38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/39.html"><!--[-->grok39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/40.html"><!--[-->grok40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/41.html"><!--[-->grok41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/42.html"><!--[-->grok42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/43.html"><!--[-->grok43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/44.html"><!--[-->grok44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/45.html"><!--[-->grok45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/46.html"><!--[-->grok46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/47.html"><!--[-->grok47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/48.html"><!--[-->grok48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/49.html"><!--[-->grok49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/50.html"><!--[-->grok50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/51.html"><!--[-->grok51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/52.html"><!--[-->grok52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/53.html"><!--[-->grok53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/54.html"><!--[-->grok54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/55.html"><!--[-->grok55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/56.html"><!--[-->grok56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/57.html"><!--[-->grok57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/58.html"><!--[-->grok58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/59.html"><!--[-->grok59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/60.html"><!--[-->grok60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/61.html"><!--[-->grok61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/62.html"><!--[-->grok62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/63.html"><!--[-->grok63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/64.html"><!--[-->grok64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/65.html"><!--[-->grok65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/66.html"><!--[-->grok66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/67.html"><!--[-->grok67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/grok/68.html"><!--[-->grok68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">wiki <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/guide/1.html"><!--[-->wiki1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/2.html"><!--[-->wiki2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/3.html"><!--[-->wiki3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/4.html"><!--[-->wiki4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/5.html"><!--[-->wiki5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/6.html"><!--[-->wiki6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/7.html"><!--[-->wiki7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/8.html"><!--[-->wiki8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/9.html"><!--[-->wiki9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/10.html"><!--[-->wiki10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/11.html"><!--[-->wiki11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/12.html"><!--[-->wiki12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/13.html"><!--[-->wiki13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/14.html"><!--[-->wiki14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/15.html"><!--[-->wiki15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/16.html"><!--[-->wiki16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/17.html"><!--[-->wiki17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/18.html"><!--[-->wiki18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/19.html"><!--[-->wiki19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/20.html"><!--[-->wiki20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/21.html"><!--[-->wiki21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/22.html"><!--[-->wiki22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/23.html"><!--[-->wiki23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/24.html"><!--[-->wiki24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/25.html"><!--[-->wiki25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/26.html"><!--[-->wiki26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/27.html"><!--[-->wiki27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/28.html"><!--[-->wiki28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/29.html"><!--[-->wiki29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/30.html"><!--[-->wiki30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/31.html"><!--[-->wiki31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/32.html"><!--[-->wiki32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/33.html"><!--[-->wiki33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/34.html"><!--[-->wiki34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/35.html"><!--[-->wiki35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/36.html"><!--[-->wiki36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/37.html"><!--[-->wiki37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/38.html"><!--[-->wiki38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/39.html"><!--[-->wiki39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/40.html"><!--[-->wiki40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/41.html"><!--[-->wiki41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/42.html"><!--[-->wiki42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/43.html"><!--[-->wiki43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/44.html"><!--[-->wiki44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/45.html"><!--[-->wiki45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/46.html"><!--[-->wiki46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/47.html"><!--[-->wiki47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/48.html"><!--[-->wiki48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/49.html"><!--[-->wiki49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/50.html"><!--[-->wiki50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/51.html"><!--[-->wiki51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/52.html"><!--[-->wiki52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/53.html"><!--[-->wiki53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/54.html"><!--[-->wiki54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/55.html"><!--[-->wiki55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/56.html"><!--[-->wiki56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/57.html"><!--[-->wiki57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/58.html"><!--[-->wiki58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/59.html"><!--[-->wiki59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/60.html"><!--[-->wiki60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/61.html"><!--[-->wiki61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/62.html"><!--[-->wiki62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/63.html"><!--[-->wiki63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/64.html"><!--[-->wiki64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/65.html"><!--[-->wiki65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/66.html"><!--[-->wiki66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/67.html"><!--[-->wiki67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/guide/68.html"><!--[-->wiki68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">deep <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/1.html"><!--[-->deep1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/2.html"><!--[-->deep2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/3.html"><!--[-->deep3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/4.html"><!--[-->deep4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/5.html"><!--[-->deep5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/6.html"><!--[-->deep6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/7.html"><!--[-->deep7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/8.html"><!--[-->deep8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/9.html"><!--[-->deep9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/10.html"><!--[-->deep10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/11.html"><!--[-->deep11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/12.html"><!--[-->deep12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/13.html"><!--[-->deep13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/14.html"><!--[-->deep14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/15.html"><!--[-->deep15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/16.html"><!--[-->deep16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/17.html"><!--[-->deep17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/18.html"><!--[-->deep18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/19.html"><!--[-->deep19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/20.html"><!--[-->deep20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/21.html"><!--[-->deep21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/22.html"><!--[-->deep22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/23.html"><!--[-->deep23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/24.html"><!--[-->deep24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/25.html"><!--[-->deep25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/26.html"><!--[-->deep26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/27.html"><!--[-->deep27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/28.html"><!--[-->deep28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/29.html"><!--[-->deep29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/30.html"><!--[-->deep30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/31.html"><!--[-->deep31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/32.html"><!--[-->deep32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/33.html"><!--[-->deep33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/34.html"><!--[-->deep34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/35.html"><!--[-->deep35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/36.html"><!--[-->deep36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/37.html"><!--[-->deep37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/38.html"><!--[-->deep38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/39.html"><!--[-->deep39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/40.html"><!--[-->deep40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/41.html"><!--[-->deep41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/42.html"><!--[-->deep42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/43.html"><!--[-->deep43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/44.html"><!--[-->deep44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/45.html"><!--[-->deep45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/46.html"><!--[-->deep46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/47.html"><!--[-->deep47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/48.html"><!--[-->deep48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/49.html"><!--[-->deep49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/50.html"><!--[-->deep50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/51.html"><!--[-->deep51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/52.html"><!--[-->deep52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/53.html"><!--[-->deep53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/54.html"><!--[-->deep54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/55.html"><!--[-->deep55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/56.html"><!--[-->deep56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/57.html"><!--[-->deep57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/58.html"><!--[-->deep58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/59.html"><!--[-->deep59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/60.html"><!--[-->deep60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/61.html"><!--[-->deep61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/62.html"><!--[-->deep62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/63.html"><!--[-->deep63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/64.html"><!--[-->deep64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/65.html"><!--[-->deep65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/66.html"><!--[-->deep66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/67.html"><!--[-->deep67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/deepseek/68.html"><!--[-->deep68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">quotes <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/1.html"><!--[-->quotes1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/2.html"><!--[-->quotes2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/3.html"><!--[-->quotes3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/4.html"><!--[-->quotes4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/5.html"><!--[-->quotes5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/6.html"><!--[-->quotes6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/7.html"><!--[-->quotes7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/8.html"><!--[-->quotes8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/9.html"><!--[-->quotes9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/10.html"><!--[-->quotes10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/11.html"><!--[-->quotes11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/12.html"><!--[-->quotes12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/13.html"><!--[-->quotes13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/14.html"><!--[-->quotes14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/15.html"><!--[-->quotes15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/16.html"><!--[-->quotes16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/17.html"><!--[-->quotes17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/18.html"><!--[-->quotes18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/19.html"><!--[-->quotes19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/20.html"><!--[-->quotes20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/21.html"><!--[-->quotes21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/22.html"><!--[-->quotes22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/23.html"><!--[-->quotes23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/24.html"><!--[-->quotes24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/25.html"><!--[-->quotes25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/26.html"><!--[-->quotes26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/27.html"><!--[-->quotes27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/28.html"><!--[-->quotes28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/29.html"><!--[-->quotes29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/30.html"><!--[-->quotes30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/31.html"><!--[-->quotes31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/32.html"><!--[-->quotes32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/33.html"><!--[-->quotes33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/34.html"><!--[-->quotes34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/35.html"><!--[-->quotes35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/36.html"><!--[-->quotes36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/37.html"><!--[-->quotes37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/38.html"><!--[-->quotes38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/39.html"><!--[-->quotes39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/40.html"><!--[-->quotes40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/41.html"><!--[-->quotes41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/42.html"><!--[-->quotes42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/43.html"><!--[-->quotes43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/44.html"><!--[-->quotes44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/45.html"><!--[-->quotes45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/46.html"><!--[-->quotes46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/47.html"><!--[-->quotes47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/48.html"><!--[-->quotes48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/49.html"><!--[-->quotes49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/50.html"><!--[-->quotes50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/51.html"><!--[-->quotes51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/52.html"><!--[-->quotes52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/53.html"><!--[-->quotes53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/54.html"><!--[-->quotes54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/55.html"><!--[-->quotes55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/56.html"><!--[-->quotes56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/57.html"><!--[-->quotes57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/58.html"><!--[-->quotes58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/59.html"><!--[-->quotes59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/60.html"><!--[-->quotes60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/61.html"><!--[-->quotes61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/62.html"><!--[-->quotes62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/63.html"><!--[-->quotes63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/64.html"><!--[-->quotes64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/65.html"><!--[-->quotes65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/66.html"><!--[-->quotes66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/67.html"><!--[-->quotes67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/quotes/68.html"><!--[-->quotes68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">chatai <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/1.html"><!--[-->chatai1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/2.html"><!--[-->chatai2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/3.html"><!--[-->chatai3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/4.html"><!--[-->chatai4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/5.html"><!--[-->chatai5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/6.html"><!--[-->chatai6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/7.html"><!--[-->chatai7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/8.html"><!--[-->chatai8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/9.html"><!--[-->chatai9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/10.html"><!--[-->chatai10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/11.html"><!--[-->chatai11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/12.html"><!--[-->chatai12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/13.html"><!--[-->chatai13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/14.html"><!--[-->chatai14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/15.html"><!--[-->chatai15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/16.html"><!--[-->chatai16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/17.html"><!--[-->chatai17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/18.html"><!--[-->chatai18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/19.html"><!--[-->chatai19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/20.html"><!--[-->chatai20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/21.html"><!--[-->chatai21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/22.html"><!--[-->chatai22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/23.html"><!--[-->chatai23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/24.html"><!--[-->chatai24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/25.html"><!--[-->chatai25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/26.html"><!--[-->chatai26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/27.html"><!--[-->chatai27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/28.html"><!--[-->chatai28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/29.html"><!--[-->chatai29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/30.html"><!--[-->chatai30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/31.html"><!--[-->chatai31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/32.html"><!--[-->chatai32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/33.html"><!--[-->chatai33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/34.html"><!--[-->chatai34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/35.html"><!--[-->chatai35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/36.html"><!--[-->chatai36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/37.html"><!--[-->chatai37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/38.html"><!--[-->chatai38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/39.html"><!--[-->chatai39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/40.html"><!--[-->chatai40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/41.html"><!--[-->chatai41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/42.html"><!--[-->chatai42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/43.html"><!--[-->chatai43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/44.html"><!--[-->chatai44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/45.html"><!--[-->chatai45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/46.html"><!--[-->chatai46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/47.html"><!--[-->chatai47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/48.html"><!--[-->chatai48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/49.html"><!--[-->chatai49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/50.html"><!--[-->chatai50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/51.html"><!--[-->chatai51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/52.html"><!--[-->chatai52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/53.html"><!--[-->chatai53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/54.html"><!--[-->chatai54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/55.html"><!--[-->chatai55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/56.html"><!--[-->chatai56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/57.html"><!--[-->chatai57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/58.html"><!--[-->chatai58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/59.html"><!--[-->chatai59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/60.html"><!--[-->chatai60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/61.html"><!--[-->chatai61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/62.html"><!--[-->chatai62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/63.html"><!--[-->chatai63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/64.html"><!--[-->chatai64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/65.html"><!--[-->chatai65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/66.html"><!--[-->chatai66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/67.html"><!--[-->chatai67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/chatai/68.html"><!--[-->chatai68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">library <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/library/1.html"><!--[-->library1<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/2.html"><!--[-->library2<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/3.html"><!--[-->library3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/4.html"><!--[-->library4<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/5.html"><!--[-->library5<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/6.html"><!--[-->library6<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/7.html"><!--[-->library7<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/8.html"><!--[-->library8<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/9.html"><!--[-->library9<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/10.html"><!--[-->library10<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/11.html"><!--[-->library11<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/12.html"><!--[-->library12<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/13.html"><!--[-->library13<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/14.html"><!--[-->library14<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/15.html"><!--[-->library15<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/16.html"><!--[-->library16<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/17.html"><!--[-->library17<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/18.html"><!--[-->library18<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/19.html"><!--[-->library19<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/20.html"><!--[-->library20<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/21.html"><!--[-->library21<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/22.html"><!--[-->library22<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/23.html"><!--[-->library23<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/24.html"><!--[-->library24<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/25.html"><!--[-->library25<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/26.html"><!--[-->library26<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/27.html"><!--[-->library27<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/28.html"><!--[-->library28<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/29.html"><!--[-->library29<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/30.html"><!--[-->library30<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/31.html"><!--[-->library31<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/32.html"><!--[-->library32<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/33.html"><!--[-->library33<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/34.html"><!--[-->library34<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/35.html"><!--[-->library35<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/36.html"><!--[-->library36<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/37.html"><!--[-->library37<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/38.html"><!--[-->library38<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/39.html"><!--[-->library39<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/40.html"><!--[-->library40<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/41.html"><!--[-->library41<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/42.html"><!--[-->library42<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/43.html"><!--[-->library43<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/44.html"><!--[-->library44<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/45.html"><!--[-->library45<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/46.html"><!--[-->library46<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/47.html"><!--[-->library47<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/48.html"><!--[-->library48<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/49.html"><!--[-->library49<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/50.html"><!--[-->library50<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/51.html"><!--[-->library51<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/52.html"><!--[-->library52<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/53.html"><!--[-->library53<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/54.html"><!--[-->library54<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/55.html"><!--[-->library55<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/56.html"><!--[-->library56<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/57.html"><!--[-->library57<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/58.html"><!--[-->library58<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/59.html"><!--[-->library59<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/60.html"><!--[-->library60<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/61.html"><!--[-->library61<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/62.html"><!--[-->library62<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/63.html"><!--[-->library63<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/64.html"><!--[-->library64<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/65.html"><!--[-->library65<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/66.html"><!--[-->library66<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/67.html"><!--[-->library67<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/library/68.html"><!--[-->library68<!--]--><!----><!----></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">ecosystem <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">website</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/partners/"><!--[-->partners<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/themes.html"><!--[-->website<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/newsletters.html"><!--[-->deepseekletters<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/navigation.html"><!--[-->AI Navigation<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/DeepSeek.html"><!--[-->DeepSeek-V3<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/ChatGPT.html"><!--[-->ChatGPT<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/Promptes.html"><!--[-->GPT Prompts<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/ecosystem/Projects.html"><!--[-->AI Projects<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.474x.com" target="_blank" rel="noopener noreferrer"><!--[-->474x.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.494x.com" target="_blank" rel="noopener noreferrer"><!--[-->494x.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.64ii.com" target="_blank" rel="noopener noreferrer"><!--[-->64ii.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.81oo.com" target="_blank" rel="noopener noreferrer"><!--[-->81oo.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">Library</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="https://e.m44m.com/" target="_blank" rel="noopener noreferrer"><!--[-->Vue Router<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://f.m44m.com/" target="_blank" rel="noopener noreferrer"><!--[-->Pinia<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.82ii.com" target="_blank" rel="noopener noreferrer"><!--[-->tool<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">Vue</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="https://g.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Vue Mastery<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://h.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Vue School<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">help</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="https://i.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Discord<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://github.com/hyaliyun/stanford" target="_blank" rel="noopener noreferrer"><!--[-->GitHub<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.z2.pw" target="_blank" rel="noopener noreferrer"><!--[-->DEV<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--[--><div class="vt-menu-group"><p class="vt-menu-group-title">stanford</p><!--[--><!--[--><a class="vt-link link vt-menu-link" href="https://c.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->blog<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://d.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Twitter<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://e.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->Activity<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://w.z2.pw" target="_blank" rel="noopener noreferrer"><!--[-->CMS<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://a.z2.pw" target="_blank" rel="noopener noreferrer"><!--[-->deepseekmagSheets<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://a.434x.com" target="_blank" rel="noopener noreferrer"><!--[-->Tailwind<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://b.434x.com" target="_blank" rel="noopener noreferrer"><!--[-->Three.js<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.q8q9.com" target="_blank" rel="noopener noreferrer"><!--[-->youtube<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/about/team.html" data-v-2cfd1945 data-v-c3f7059f><!--[-->team<!--]--><!----><!----></a><!--]--><!--[--><div class="vt-flyout VPNavBarMenuGroup" data-v-2cfd1945 data-v-0c5978fc><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false"><!--[--><span class="vt-flyout-button-text">show <!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-text-icon"><path d="M12,16c-0.3,0-0.5-0.1-0.7-0.3l-6-6c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l5.3,5.3l5.3-5.3c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-6,6C12.5,15.9,12.3,16,12,16z"></path></svg></span><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><div class="vt-menu-items"><!--[--><!--[--><a class="vt-link link vt-menu-link" href="/drive/donation.html"><!--[-->donation<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/PromptLibrary.html"><!--[-->PromptLibrary<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/prompt.html"><!--[-->prompt<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/aiprompt.html"><!--[-->Vertex AI<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/deep.html"><!--[-->deepseekStarted<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/gpt.html"><!--[-->ChatGPT<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/drive/team.html"><!--[-->crypto<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="/partners/"><!--[-->partners<!--]--><!----><!----></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://www.3kk3.com" target="_blank" rel="noopener noreferrer"><!--[-->3kk3.com<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://b.q8q9.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseek<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://c.4s5s.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr1<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://b.6n7n.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr2<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://f.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr3<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://c.q8q9.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr4<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://a.l00m.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr5<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--[--><a class="vt-link link vt-menu-link" href="https://g.m44m.com" target="_blank" rel="noopener noreferrer"><!--[-->deepseekr6<!--]--><!----><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><!--]--><!--]--></div><!--[--><!--]--></div></div></div><!--]--><!--[--><a class="vt-link link VPNavBarMenuLink" href="/swap/app.html" data-v-2cfd1945 data-v-c3f7059f><!--[-->swap<!--]--><!----><!----></a><!--]--><!--]--><div class="vt-flyout VPNavBarMenuGroup active VPNavBarLocale" data-v-2cfd1945 data-v-802bec0f><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false" aria-label="Select Language"><!--[--><div class="vt-locales-btn-icon-container" data-v-802bec0f><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-locales-btn-icon" data-v-802bec0f><path d="M0 0h24v24H0z" fill="none"></path><path d=" M12.87 15.07l-2.54-2.51.03-.03c1.74-1.94 2.98-4.17 3.71-6.53H17V4h-7V2H8v2H1v1.99h11.17C11.5 7.92 10.44 9.75 9 11.35 8.07 10.32 7.3 9.19 6.69 8h-2c.73 1.63 1.73 3.17 2.98 4.56l-5.09 5.02L4 19l5-5 3.11 3.11.76-2.04zM18.5 10h-2L12 22h2l1.12-3h4.75L21 22h2l-4.5-12zm-2.62 7l1.62-4.33L19.12 17h-3.24z " class="css-c4d79v"></path></svg></div><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><!----><!--[--><!--[--><!--[--><div class="vt-menu-items x-padding" data-v-802bec0f><!--[--><div class="vt-locales-menu-item"><a href="https://g.m44m.com/grok/27.html" target="_blank" class="vt-locales-menu-item-text"> <svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" height="24px" viewbox="0 0 24 24" width="24px" class="vt-link-icon"><path d="M0 0h24v24H0V0z" fill="none"></path><path d="M9 5v2h6.59L4 18.59 5.41 20 17 8.41V15h2V5H9z"></path></svg></a><a href="https://github.com/hyaliyun/stanford" title=" Repository" target="_blank" class="vt-locales-btn-icon-container"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-locales-btn-icon repo"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg></a></div><!--]--></div><!----><!--]--><!--]--><!--]--></div></div></div></nav><div class="VPNavBarAppearance appearance" data-v-78ea45ed data-v-7e4f86a7><button class="vt-switch vt-switch-appearance" type="button" role="switch" aria-label="Toggle dark mode" aria-checked="false" data-v-7e4f86a7><span class="vt-switch-check"><span class="vt-switch-icon"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-sun"><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-moon"><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div><div class="vt-social-links VPNavBarSocialLinks social-links" data-v-78ea45ed data-v-44bed5da><!--[--><a class="vt-social-link is-small" href="https://github.com/hyaliyun/stanford/" title="github" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-social-link-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg><span class="visually-hidden">github</span></a><!--]--></div><div class="vt-flyout VPNavBarExtra extra" data-v-78ea45ed data-v-d9c85796><button type="button" class="vt-flyout-button" aria-haspopup="true" aria-expanded="false" aria-label="extra navigation"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-flyout-button-icon"><circle cx="12" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="5" cy="12" r="2"></circle></svg><!--]--></button><div class="vt-flyout-menu"><div class="vt-menu"><!----><!--[--><!--[--><div class="vt-menu-group" data-v-d9c85796><div class="vt-menu-item item" data-v-d9c85796><p class="vt-menu-label" data-v-d9c85796>Appearance</p><div class="vt-menu-action action" data-v-d9c85796><button class="vt-switch vt-switch-appearance" type="button" role="switch" aria-label="Toggle dark mode" aria-checked="false" data-v-d9c85796><span class="vt-switch-check"><span class="vt-switch-icon"><!--[--><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-sun"><path d="M12,18c-3.3,0-6-2.7-6-6s2.7-6,6-6s6,2.7,6,6S15.3,18,12,18zM12,8c-2.2,0-4,1.8-4,4c0,2.2,1.8,4,4,4c2.2,0,4-1.8,4-4C16,9.8,14.2,8,12,8z"></path><path d="M12,4c-0.6,0-1-0.4-1-1V1c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,3.6,12.6,4,12,4z"></path><path d="M12,24c-0.6,0-1-0.4-1-1v-2c0-0.6,0.4-1,1-1s1,0.4,1,1v2C13,23.6,12.6,24,12,24z"></path><path d="M5.6,6.6c-0.3,0-0.5-0.1-0.7-0.3L3.5,4.9c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C6.2,6.5,5.9,6.6,5.6,6.6z"></path><path d="M19.8,20.8c-0.3,0-0.5-0.1-0.7-0.3l-1.4-1.4c-0.4-0.4-0.4-1,0-1.4s1-0.4,1.4,0l1.4,1.4c0.4,0.4,0.4,1,0,1.4C20.3,20.7,20,20.8,19.8,20.8z"></path><path d="M3,13H1c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S3.6,13,3,13z"></path><path d="M23,13h-2c-0.6,0-1-0.4-1-1s0.4-1,1-1h2c0.6,0,1,0.4,1,1S23.6,13,23,13z"></path><path d="M4.2,20.8c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C4.7,20.7,4.5,20.8,4.2,20.8z"></path><path d="M18.4,6.6c-0.3,0-0.5-0.1-0.7-0.3c-0.4-0.4-0.4-1,0-1.4l1.4-1.4c0.4-0.4,1-0.4,1.4,0s0.4,1,0,1.4l-1.4,1.4C18.9,6.5,18.6,6.6,18.4,6.6z"></path></svg><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-switch-appearance-moon"><path d="M12.1,22c-0.3,0-0.6,0-0.9,0c-5.5-0.5-9.5-5.4-9-10.9c0.4-4.8,4.2-8.6,9-9c0.4,0,0.8,0.2,1,0.5c0.2,0.3,0.2,0.8-0.1,1.1c-2,2.7-1.4,6.4,1.3,8.4c2.1,1.6,5,1.6,7.1,0c0.3-0.2,0.7-0.3,1.1-0.1c0.3,0.2,0.5,0.6,0.5,1c-0.2,2.7-1.5,5.1-3.6,6.8C16.6,21.2,14.4,22,12.1,22zM9.3,4.4c-2.9,1-5,3.6-5.2,6.8c-0.4,4.4,2.8,8.3,7.2,8.7c2.1,0.2,4.2-0.4,5.8-1.8c1.1-0.9,1.9-2.1,2.4-3.4c-2.5,0.9-5.3,0.5-7.5-1.1C9.2,11.4,8.1,7.7,9.3,4.4z"></path></svg><!--]--></span></span></button></div></div></div><div class="vt-menu-group" data-v-d9c85796><div class="vt-menu-item item" data-v-d9c85796><div class="vt-social-links social-links" data-v-d9c85796><!--[--><a class="vt-social-link is-small" href="https://github.com/hyaliyun/stanford/" title="github" target="_blank" rel="noopener noreferrer"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" viewbox="0 0 24 24" class="vt-social-link-icon"><path d="M12 .297c-6.63 0-12 5.373-12 12 0 5.303 3.438 9.8 8.205 11.385.6.113.82-.258.82-.577 0-.285-.01-1.04-.015-2.04-3.338.724-4.042-1.61-4.042-1.61C4.422 18.07 3.633 17.7 3.633 17.7c-1.087-.744.084-.729.084-.729 1.205.084 1.838 1.236 1.838 1.236 1.07 1.835 2.809 1.305 3.495.998.108-.776.417-1.305.76-1.605-2.665-.3-5.466-1.332-5.466-5.93 0-1.31.465-2.38 1.235-3.22-.135-.303-.54-1.523.105-3.176 0 0 1.005-.322 3.3 1.23.96-.267 1.98-.399 3-.405 1.02.006 2.04.138 3 .405 2.28-1.552 3.285-1.23 3.285-1.23.645 1.653.24 2.873.12 3.176.765.84 1.23 1.91 1.23 3.22 0 4.61-2.805 5.625-5.475 5.92.42.36.81 1.096.81 2.22 0 1.606-.015 2.896-.015 3.286 0 .315.21.69.825.57C20.565 22.092 24 17.592 24 12.297c0-6.627-5.373-12-12-12"></path></svg><span class="visually-hidden">github</span></a><!--]--></div></div></div><!--]--><!--]--></div></div></div><button type="button" class="vt-hamburger VPNavBarHamburger hamburger" aria-label="Mobile navigation" aria-expanded="false" aria-controls="VPNavScreen" data-v-78ea45ed data-v-dcc88df6><span class="vt-hamburger-container"><span class="vt-hamburger-top"></span><span class="vt-hamburger-middle"></span><span class="vt-hamburger-bottom"></span></span></button></div></div></div><!----></header><!----><!----><div id="VPContent" class="VPContent" data-v-e4982c5a data-v-8b82bdb3><div class="VPContentPage" data-v-8b82bdb3><main><div style="position:relative;"><div><section data-v-2262462c><div class="top-banner" data-v-2262462c><div class="top-banner-title" data-v-2262462c><div class="top-banner-title-text" data-v-2262462c>prompts chat</div></div></div><div class="search-container" data-v-2262462c><span class="search-icon" data-v-2262462c></span><input type="text" class="search-input" value="" placeholder="Search..." data-v-2262462c><!----></div><div class="card-container" data-v-2262462c><!--[--><div class="poem-container" data-v-2262462c data-v-9c550d47><div class="review" data-v-9c550d47><div class="review-title" data-v-9c550d47><span class="icon" data-v-9c550d47>question:</span>Create a gradio webUI to run the model created using this code, **code**: import torch import torch.nn as nn import torch.nn.functional as F from torch.nn.utils.rnn import pad_sequence from torch.utils.data import DataLoader, Dataset from collections import Counter import json from tqdm import tqdm import math # Check if CUDA is available device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) def positional_encoding(seq_len, d_model, device): pos = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).to(device) pe = torch.zeros(seq_len, d_model, device=device) pe[:, 0::2] = torch.sin(pos * div_term) pe[:, 1::2] = torch.cos(pos * div_term) return pe.unsqueeze(0) # Expert Transformer Model class TransformerExpert(nn.Module): def __init__(self, input_size, d_model, output_size, nhead, dim_feedforward, num_encoder_layers=1): super(TransformerExpert, self).__init__() self.d_model = d_model self.input_fc = nn.Linear(input_size, d_model) encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True) self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers) self.output_fc = nn.Linear(d_model, output_size) def forward(self, x): x = self.input_fc(x) + positional_encoding(x.size(1), self.d_model, x.device) transformer_output = self.transformer_encoder(x) output = self.output_fc(transformer_output) # Apply output_fc to each time step in the sequence return output # Gating Network class GatingNetwork(nn.Module): def __init__(self, input_feature_dim, num_experts, hidden_dims=None, dropout_rate=0.0): super(GatingNetwork, self).__init__() layers = [] last_dim = input_feature_dim # If hidden layers are specified, create them if hidden_dims is not None: for hidden_dim in hidden_dims: layers.append(nn.Linear(last_dim, hidden_dim)) layers.append(nn.ReLU()) # You could make this a hyperparameter as well if dropout_rate &gt; 0.0: layers.append(nn.Dropout(dropout_rate)) last_dim = hidden_dim # Final layer projecting to the number of experts layers.append(nn.Linear(last_dim, num_experts)) self.fc_layers = nn.Sequential(*layers) self.softmax = nn.Softmax(dim=1) def forward(self, x): # Assuming x is of shape [batch_size, seq_len, d_model], aggregate across the sequence length x = x.mean(dim=1) # Aggregate feature per instance x = self.fc_layers(x) # Pass through gating network layers return self.softmax(x) # Define hyperparameters specific to the transformer d_model = 384 #128 nhead = 8 #8 dim_feedforward = 768 #256 num_encoder_layers = 8 #2 num_experts = 2 #2 hidden_dims = [512, 256] # List of hidden layer sizes dropout_rate = 0.5 # Dropout rate gating_network = GatingNetwork( input_feature_dim=d_model, num_experts=num_experts, hidden_dims=hidden_dims, dropout_rate=dropout_rate, ) # Mixture of Experts Model class MixtureOfTransformerExperts(nn.Module): def __init__(self, input_size, d_model, output_size, nhead, dim_feedforward, num_experts, num_encoder_layers=1): super(MixtureOfTransformerExperts, self).__init__() self.num_experts = num_experts self.output_size = output_size # Store output_size as an instance variable self.experts = nn.ModuleList([TransformerExpert(input_size, d_model, output_size, nhead, dim_feedforward, num_encoder_layers) for _ in range(num_experts)]) self.gating_network = GatingNetwork(d_model, num_experts) def forward(self, x): gating_scores = self.gating_network(x) # [batch_size, num_experts] expert_outputs = [expert(x) for expert in self.experts] # List of [batch_size, seq_len, output_size] stacked_expert_outputs = torch.stack(expert_outputs) # Shape: [num_experts, batch_size, seq_len, output_size] # Expand gating scores expanded_gating_scores = gating_scores.unsqueeze(2).unsqueeze(3) # Shape: [batch_size, num_experts, 1, 1] expanded_gating_scores = expanded_gating_scores.expand(-1, -1, x.size(1), self.output_size) expanded_gating_scores = expanded_gating_scores.transpose(0, 1) # Shape: [num_experts, batch_size, seq_len, output_size] # Now the shape of expanded_gating_scores matches stacked_expert_outputs, and broadcasting will work mixed_output = torch.sum(stacked_expert_outputs * expanded_gating_scores, dim=0) # Sum weighted expert outputs for each time step return mixed_output class QAJsonlDataset(Dataset): def __init__(self, path, seq_len): self.seq_len = seq_len self.pairs = self.load_data(path) # Flatten the pairs completely before passing them to build_vocab self.vocab, self.idx2token = self.build_vocab([word for pair in self.pairs for sublist in pair for word in sublist]) self.tokenized_pairs = [(self.tokenize(q), self.tokenize(a)) for q, a in self.pairs] def load_data(self, path): pairs = [] with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: for line in f: data = json.loads(line.strip()) question, answer = data.get(&quot;question&quot;, &quot;&quot;), data.get(&quot;answer&quot;, &quot;&quot;) pairs.append((question.split(), answer.split())) return pairs def tokenize(self, words): # Tokenize a sentence and pad if necessary # Add &lt;eos&gt; token at the end if theres room tokens = [self.vocab.get(w, self.vocab[&quot;&lt;unk&gt;&quot;]) for w in words] if len(tokens) &lt; self.seq_len: tokens.append(self.vocab[&quot;&lt;eos&gt;&quot;]) # Add &lt;eos&gt; token tokens.extend([self.vocab[&quot;&lt;pad&gt;&quot;]] * (self.seq_len - len(tokens))) # Pad the rest else: tokens = tokens[:self.seq_len - 1] + [self.vocab[&quot;&lt;eos&gt;&quot;]] return tokens def build_vocab(self, words): # Start with special tokens with fixed indices vocab = {&quot;&lt;unk&gt;&quot;: 0, &quot;&lt;pad&gt;&quot;: 1, &quot;&lt;eos&gt;&quot;: 2} start_index = len(vocab) # Use Counter to count word frequencies in the corpus counts = Counter(words) # Create the vocab dictionary with all words, starting indices after the special tokens for word, _ in counts.most_common(): if word not in vocab: # Skip special tokens vocab[word] = len(vocab) # Create the reverse mapping from indices to words idx2token = {idx: token for token, idx in vocab.items()} return vocab, idx2token def __len__(self): return len(self.tokenized_pairs) def __getitem__(self, idx): tokenized_question, tokenized_answer = self.tokenized_pairs[idx] return torch.tensor(tokenized_question, dtype=torch.long), torch.tensor(tokenized_answer, dtype=torch.long) class MoETransformerModel(nn.Module): def __init__(self, vocab_size, d_model, moe): super(MoETransformerModel, self).__init__() self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model) self.moe = moe self.dropout = nn.Dropout(p=0.125) # Dropout added for regularization def forward(self, x): embedded = self.dropout(self.embedding(x)) return self.moe(embedded) # Remove positional encoding addition here, as its already added in TransformerExpert def collate_fn(batch): questions, answers = zip(*batch) questions = pad_sequence(questions, batch_first=True, padding_value=0) answers = pad_sequence(answers, batch_first=True, padding_value=0) return questions, answers # Set the path to your text file and define sequence length path_to_text = &#39;train.jsonl&#39; # replace with the path to your text file seq_len = 24 # sequence length # Create a dataset and data loader dataset = QAJsonlDataset(path_to_text, seq_len) data_loader = DataLoader(dataset, batch_size=24, shuffle=True, collate_fn=collate_fn, pin_memory=True) # Training loop - added gradient clipping to avoid exploding gradients def train_model(model, criterion, optimizer, num_epochs, data_loader): model.train() for epoch in range(num_epochs): total_loss = 0 progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f&quot;Epoch {epoch+1}&quot;, leave=False) for i, (inputs, targets) in progress_bar: inputs, targets = inputs.to(device), targets.to(device) optimizer.zero_grad() predictions = model(inputs) predictions = predictions.view(-1, predictions.size(-1)) targets = targets.view(-1) # Flatten the targets to [batch_size * seq_len] loss = criterion(predictions, targets) loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping optimizer.step() total_loss += loss.item() progress_bar.set_postfix({&quot;Loss&quot;: loss.item()}) average_loss = total_loss / len(data_loader.dataset) print(f&quot;Epoch {epoch+1}, Average Loss: {average_loss}&quot;) def generate_text(model, dataset, seed_text, num_generate, temperature=1.0): model.eval() # Put the model in evaluation mode # List to store the generated tokens generated_tokens = [] # Initial sequence (prefix) to start the generation process input_sequence = [dataset.vocab.get(word, dataset.vocab[&quot;&lt;pad&gt;&quot;]) for word in seed_text.split()] # Convert to token IDs current_sequence = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0) current_sequence = current_sequence.to(device) # Generate num_generate tokens for _ in range(num_generate): # Forward pass through the model with torch.no_grad(): output = model(current_sequence) # Get probabilities, apply temperature scaling, and sample from the distribution probabilities = F.softmax(output[:, -1, :] / temperature, dim=-1).detach() next_token_idx = torch.multinomial(probabilities, 1).item() # Append token to the current sequence and to the generated tokens generated_tokens.append(next_token_idx) current_sequence = torch.cat((current_sequence, torch.tensor([[next_token_idx]])), 1).to(device) # Convert tokens to words generated_text = &quot; &quot;.join([dataset.idx2token.get(token, &quot;&lt;unk&gt;&quot;) for token in generated_tokens]) # Use .get() to provide a default value for missing keys return generated_text # Function to count the number of tokens in the dataset def count_tokens_in_dataset(dataset): return sum([len(pair[0]) + len(pair[1]) for pair in dataset.pairs]) num_tokens = count_tokens_in_dataset(dataset) print(f&quot;Total number of tokens in the dataset: {num_tokens}&quot;) vocab_size = len(dataset.vocab) # Assume dataset.vocab is defined in the QAJsonlDataset class # Instantiate resulting MoE transformer model and move it to device moe = MixtureOfTransformerExperts( input_size=d_model, d_model=d_model, output_size=vocab_size, nhead=nhead, dim_feedforward=dim_feedforward, num_experts=num_experts, num_encoder_layers=num_encoder_layers ).to(device) # Instantiate the MoE transformer model and move it to device moe_transformer_model = MoETransformerModel(vocab_size, d_model, moe).to(device) def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad) # Example usage with your model: total_params = count_parameters(moe_transformer_model) print(f&quot;Total trainable parameters: {total_params}&quot;) # Training parameters num_epochs = 2 learning_rate = 0.00045 # Define Loss Function and Optimizer for MoE model - using Label Smoothing for better generalization criterion = nn.CrossEntropyLoss(label_smoothing=0.1) optimizer = torch.optim.AdamW(moe_transformer_model.parameters(), lr=learning_rate, weight_decay=0.01) # Using AdamW with weight decay # Train the model with the text data train_model(moe_transformer_model, criterion, optimizer, num_epochs, data_loader) # Start a loop for the interactive chat-like text generation while True: try: # Get user input seed_text = input(&quot;Enter seed text (type &#39;quit&#39; to stop): &quot;) # Check if user wants to quit the interaction if seed_text.lower() == &quot;quit&quot;: print(&quot;Exiting text generation chat.&quot;) break # User input is not empty and not quit, generate text if seed_text.strip(): num_generate = 20 # Number of words to generate temperature = 1.0 # Sampling temperature, higher will increase diversity # Use the trained model to generate text generated_text = generate_text(moe_transformer_model, dataset, seed_text, num_generate, temperature) print(&quot;Generated Text:&quot;, generated_text) else: print(&quot;Seed text cannot be empty.&quot;) except KeyboardInterrupt: # Handle KeyboardInterrupt (Ctrl+C) to gracefully exit print(&quot;nExiting text generation chat.&quot;) break torch.save(moe_transformer_model.state_dict(), &quot;MoE_Transformer-X4-78M-QA.pth&quot;)</div><div class="divider" data-v-9c550d47></div><p class="review-content" data-v-9c550d47><span class="icon" data-v-9c550d47>answer:</span>To create a Gradio web UI for your model, the main steps would include: - Installing the Gradio library - Defining an inference function that the UI will use to generate responses - Creating the Gradio interface with appropriate input and output components - Launching the Gradio app Here&#39;s a Python script that includes the previous code and adds the necessary parts to create the Gradio UI: ```python # Install the Gradio library (uncomment the next line if Gradio is not installed) # !pip install gradio import gradio as gr # (Your existing model code) # You might want to reduce the following hyperparameters if you&#39;re running on a CPU to make it faster num_epochs = 1 # Use less epochs for quicker setup seq_len = 24 # sequence length # Load your trained model - Make sure to adjust path trained_model_path = &quot;MoE_Transformer-X4-78M-QA.pth&quot; # replace with the actual path to your trained model weights moe_transformer_model.load_state_dict(torch.load(trained_model_path, map_location=device)) # Define a function to be used by the interface def textual_response(seed_text): num_generate = 20 # Number of words to generate temperature = 1.0 # Sampling temperature generated_text = generate_text(moe_transformer_model, dataset, seed_text, num_generate, temperature) return generated_text # Define the Gradio interface iface = gr.Interface( fn=textual_response, inputs=gr.inputs.Textbox(lines=2, placeholder=&quot;Enter seed text here...&quot;), outputs=gr.outputs.Textbox(), title=&quot;MoE Transformer Expert Text Generation&quot;, description=&quot;Enter a seed text to generate a sequence from the trained MoE Transformer Expert&quot; ) # Launch the app - running on the local machine iface.launch() # Note: make sure the model and the additional code you need for the inference are loaded # If the model takes a lot of time to load, consider loading it outside the textual_response function to load it only once. ``` Please keep in mind that for the snippet provided to work seamlessly, you must ensure that the Gradio library is installed (`!pip install gradio`) and that you adjust the `trained_model_path` to the actual path where your trained model weights are stored. The `textual_response` function serves as an interface to call the text generation function `generate_text` with your input from the web UI. The Gradio UI will provide a textbox for you to input your seed text and after submission will show the generated text in another textbox. After running the above code, Gradio will provide a local URL to access the web UI from your browser. If you want the app to be accessible from the public link, you can set `share=True` in the `iface.launch()` call.</p></div></div><div class="poem-container" data-v-2262462c data-v-9c550d47><div class="review" data-v-9c550d47><div class="review-title" data-v-9c550d47><span class="icon" data-v-9c550d47>question:</span>Is this code functional now: import gradio as gr import torch import torch.nn as nn import torch.nn.functional as F from torch.nn.utils.rnn import pad_sequence from torch.utils.data import DataLoader, Dataset from collections import Counter import json from tqdm import tqdm import math # (Your existing model code) # Check if CUDA is available device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) def positional_encoding(seq_len, d_model, device): pos = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).to(device) pe = torch.zeros(seq_len, d_model, device=device) pe[:, 0::2] = torch.sin(pos * div_term) pe[:, 1::2] = torch.cos(pos * div_term) return pe.unsqueeze(0) # Expert Transformer Model class TransformerExpert(nn.Module): def __init__(self, input_size, d_model, output_size, nhead, dim_feedforward, num_encoder_layers=1): super(TransformerExpert, self).__init__() self.d_model = d_model self.input_fc = nn.Linear(input_size, d_model) encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True) self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers) self.output_fc = nn.Linear(d_model, output_size) def forward(self, x): x = self.input_fc(x) + positional_encoding(x.size(1), self.d_model, x.device) transformer_output = self.transformer_encoder(x) output = self.output_fc(transformer_output) # Apply output_fc to each time step in the sequence return output # Gating Network class GatingNetwork(nn.Module): def __init__(self, input_feature_dim, num_experts, hidden_dims=None, dropout_rate=0.0): super(GatingNetwork, self).__init__() layers = [] last_dim = input_feature_dim # If hidden layers are specified, create them if hidden_dims is not None: for hidden_dim in hidden_dims: layers.append(nn.Linear(last_dim, hidden_dim)) layers.append(nn.ReLU()) # You could make this a hyperparameter as well if dropout_rate &gt; 0.0: layers.append(nn.Dropout(dropout_rate)) last_dim = hidden_dim # Final layer projecting to the number of experts layers.append(nn.Linear(last_dim, num_experts)) self.fc_layers = nn.Sequential(*layers) self.softmax = nn.Softmax(dim=1) def forward(self, x): # Assuming x is of shape [batch_size, seq_len, d_model], aggregate across the sequence length x = x.mean(dim=1) # Aggregate feature per instance x = self.fc_layers(x) # Pass through gating network layers return self.softmax(x) # Define hyperparameters specific to the transformer d_model = 384 #128 nhead = 8 #8 dim_feedforward = 768 #256 num_encoder_layers = 8 #2 num_experts = 2 #2 hidden_dims = [512, 256] # List of hidden layer sizes dropout_rate = 0.5 # Dropout rate gating_network = GatingNetwork( input_feature_dim=d_model, num_experts=num_experts, hidden_dims=hidden_dims, dropout_rate=dropout_rate, ) # Mixture of Experts Model class MixtureOfTransformerExperts(nn.Module): def __init__(self, input_size, d_model, output_size, nhead, dim_feedforward, num_experts, num_encoder_layers=1): super(MixtureOfTransformerExperts, self).__init__() self.num_experts = num_experts self.output_size = output_size # Store output_size as an instance variable self.experts = nn.ModuleList([TransformerExpert(input_size, d_model, output_size, nhead, dim_feedforward, num_encoder_layers) for _ in range(num_experts)]) self.gating_network = GatingNetwork(d_model, num_experts) def forward(self, x): gating_scores = self.gating_network(x) # [batch_size, num_experts] expert_outputs = [expert(x) for expert in self.experts] # List of [batch_size, seq_len, output_size] stacked_expert_outputs = torch.stack(expert_outputs) # Shape: [num_experts, batch_size, seq_len, output_size] # Expand gating scores expanded_gating_scores = gating_scores.unsqueeze(2).unsqueeze(3) # Shape: [batch_size, num_experts, 1, 1] expanded_gating_scores = expanded_gating_scores.expand(-1, -1, x.size(1), self.output_size) expanded_gating_scores = expanded_gating_scores.transpose(0, 1) # Shape: [num_experts, batch_size, seq_len, output_size] # Now the shape of expanded_gating_scores matches stacked_expert_outputs, and broadcasting will work mixed_output = torch.sum(stacked_expert_outputs * expanded_gating_scores, dim=0) # Sum weighted expert outputs for each time step return mixed_output class QAJsonlDataset(Dataset): def __init__(self, path, seq_len): self.seq_len = seq_len self.pairs = self.load_data(path) # Flatten the pairs completely before passing them to build_vocab self.vocab, self.idx2token = self.build_vocab([word for pair in self.pairs for sublist in pair for word in sublist]) self.tokenized_pairs = [(self.tokenize(q), self.tokenize(a)) for q, a in self.pairs] def load_data(self, path): pairs = [] with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: for line in f: data = json.loads(line.strip()) question, answer = data.get(&quot;question&quot;, &quot;&quot;), data.get(&quot;answer&quot;, &quot;&quot;) pairs.append((question.split(), answer.split())) return pairs def tokenize(self, words): # Tokenize a sentence and pad if necessary # Add &lt;eos&gt; token at the end if theres room tokens = [self.vocab.get(w, self.vocab[&quot;&lt;unk&gt;&quot;]) for w in words] if len(tokens) &lt; self.seq_len: tokens.append(self.vocab[&quot;&lt;eos&gt;&quot;]) # Add &lt;eos&gt; token tokens.extend([self.vocab[&quot;&lt;pad&gt;&quot;]] * (self.seq_len - len(tokens))) # Pad the rest else: tokens = tokens[:self.seq_len - 1] + [self.vocab[&quot;&lt;eos&gt;&quot;]] return tokens def build_vocab(self, words): # Start with special tokens with fixed indices vocab = {&quot;&lt;unk&gt;&quot;: 0, &quot;&lt;pad&gt;&quot;: 1, &quot;&lt;eos&gt;&quot;: 2} start_index = len(vocab) # Use Counter to count word frequencies in the corpus counts = Counter(words) # Create the vocab dictionary with all words, starting indices after the special tokens for word, _ in counts.most_common(): if word not in vocab: # Skip special tokens vocab[word] = len(vocab) # Create the reverse mapping from indices to words idx2token = {idx: token for token, idx in vocab.items()} return vocab, idx2token def __len__(self): return len(self.tokenized_pairs) def __getitem__(self, idx): tokenized_question, tokenized_answer = self.tokenized_pairs[idx] return torch.tensor(tokenized_question, dtype=torch.long), torch.tensor(tokenized_answer, dtype=torch.long) class MoETransformerModel(nn.Module): def __init__(self, vocab_size, d_model, moe): super(MoETransformerModel, self).__init__() self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model) self.moe = moe self.dropout = nn.Dropout(p=0.125) # Dropout added for regularization def forward(self, x): embedded = self.dropout(self.embedding(x)) return self.moe(embedded) # Remove positional encoding addition here, as its already added in TransformerExpert def collate_fn(batch): questions, answers = zip(*batch) questions = pad_sequence(questions, batch_first=True, padding_value=0) answers = pad_sequence(answers, batch_first=True, padding_value=0) return questions, answers # Set the path to your text file and define sequence length path_to_text = &#39;train.jsonl&#39; # replace with the path to your text file seq_len = 24 # sequence length # Create a dataset and data loader dataset = QAJsonlDataset(path_to_text, seq_len) data_loader = DataLoader(dataset, batch_size=24, shuffle=True, collate_fn=collate_fn, pin_memory=True) def generate_text(model, dataset, seed_text, num_generate, temperature=1.0): model.eval() # Put the model in evaluation mode # List to store the generated tokens generated_tokens = [] # Initial sequence (prefix) to start the generation process input_sequence = [dataset.vocab.get(word, dataset.vocab[&quot;&lt;pad&gt;&quot;]) for word in seed_text.split()] # Convert to token IDs current_sequence = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0) current_sequence = current_sequence.to(device) # Generate num_generate tokens for _ in range(num_generate): # Forward pass through the model with torch.no_grad(): output = model(current_sequence) # Get probabilities, apply temperature scaling, and sample from the distribution probabilities = F.softmax(output[:, -1, :] / temperature, dim=-1).detach() next_token_idx = torch.multinomial(probabilities, 1).item() # Append token to the current sequence and to the generated tokens generated_tokens.append(next_token_idx) current_sequence = torch.cat((current_sequence, torch.tensor([[next_token_idx]])), 1).to(device) # Convert tokens to words generated_text = &quot; &quot;.join([dataset.idx2token.get(token, &quot;&lt;unk&gt;&quot;) for token in generated_tokens]) # Use .get() to provide a default value for missing keys return generated_text vocab_size = len(dataset.vocab) # Assume dataset.vocab is defined in the QAJsonlDataset class # Instantiate resulting MoE transformer model and move it to device moe = MixtureOfTransformerExperts( input_size=d_model, d_model=d_model, output_size=vocab_size, nhead=nhead, dim_feedforward=dim_feedforward, num_experts=num_experts, num_encoder_layers=num_encoder_layers ).to(device) # Instantiate the MoE transformer model and move it to device moe_transformer_model = MoETransformerModel(vocab_size, d_model, moe).to(device) # You might want to reduce the following hyperparameters if youre running on a CPU to make it faster num_epochs = 1 # Use less epochs for quicker setup seq_len = 24 # sequence length # Load your trained model - Make sure to adjust path trained_model_path = &quot;MoE_Transformer-X4-78M-QA.pth&quot; # replace with the actual path to your trained model weights moe_transformer_model.load_state_dict(torch.load(trained_model_path, map_location=device)) # Define a function to be used by the interface def textual_response(seed_text): num_generate = 20 # Number of words to generate temperature = 1.0 # Sampling temperature generated_text = generate_text(moe_transformer_model, dataset, seed_text, num_generate, temperature) return generated_text # Define the Gradio interface iface = gr.Interface( fn=textual_response, inputs=gr.inputs.Textbox(lines=2, placeholder=&quot;Enter seed text here&quot;), outputs=gr.outputs.Textbox(), title=&quot;MoE Transformer Expert Text Generation&quot;, description=&quot;Enter a seed text to generate a sequence from the trained MoE Transformer Expert&quot; ) # Launch the app - running on the local machine iface.launch()</div><div class="divider" data-v-9c550d47></div><p class="review-content" data-v-9c550d47><span class="icon" data-v-9c550d47>answer:</span>Yes, the code you&#39;ve provided sets up the necessary components for a Gradio web interface to interact with the &quot;Mixture of Transformer Experts&quot; (MoE) model by implementing a `textual_response` function that takes user input, generates text with the trained model, and then displays the output. However, to ensure that the code is fully functional, you need to confirm a few things: 1. Ensure you have Gradio installed. If not, you should install it using pip `pip install gradio`. 2. Ensure that the `trained_model_path` is set correctly to the location of your trained model weights. Without this, the model cannot load the necessary weights and will not work. 3. The code assumes that there is a `train.jsonl` file located in the same directory that the script is run from. You must ensure that this file is there and is correctly formatted, otherwise, youll get file not found errors or data formatting issues. 4. Keep in mind that if the code is run directly as-is without having a trained model file named &quot;MoE_Transformer-X4-78M-QA.pth&quot;, it will throw an error as it won&#39;t be able to load the model. If you meet all the above conditions, and the rest of the model training and text generation logic is correct, the code is functional and should provide a simple web UI that takes seed text as input and shows generated text as output. Remember that to use this code in practice, you would typically first train the MoE transformer model using the rest of the included code blocks, save the trained model weights, and then load those weights with the provided Gradio interface code. If there were any errors or problems while running the code, please provide the error messages, and I can help you debug the issues further.</p></div></div><div class="poem-container" data-v-2262462c data-v-9c550d47><div class="review" data-v-9c550d47><div class="review-title" data-v-9c550d47><span class="icon" data-v-9c550d47>question:</span>Modify the following code to save the vocabulary and configuration of the model to appropriate files &quot;txt&quot; for vocab and &quot;json&quot; for config, **code** : import torch import torch.nn as nn import torch.nn.functional as F from torch.nn.utils.rnn import pad_sequence from torch.utils.data import DataLoader, Dataset from collections import Counter import json from tqdm import tqdm import math # Check if CUDA is available device = torch.device(&quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;) def positional_encoding(seq_len, d_model, device): pos = torch.arange(seq_len, dtype=torch.float, device=device).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).to(device) pe = torch.zeros(seq_len, d_model, device=device) pe[:, 0::2] = torch.sin(pos * div_term) pe[:, 1::2] = torch.cos(pos * div_term) return pe.unsqueeze(0) # Expert Transformer Model class TransformerExpert(nn.Module): def __init__(self, input_size, d_model, output_size, nhead, dim_feedforward, num_encoder_layers=1): super(TransformerExpert, self).__init__() self.d_model = d_model self.input_fc = nn.Linear(input_size, d_model) encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, dim_feedforward=dim_feedforward, batch_first=True) self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_encoder_layers) self.output_fc = nn.Linear(d_model, output_size) def forward(self, x): x = self.input_fc(x) + positional_encoding(x.size(1), self.d_model, x.device) transformer_output = self.transformer_encoder(x) output = self.output_fc(transformer_output) # Apply output_fc to each time step in the sequence return output # Gating Network class GatingNetwork(nn.Module): def __init__(self, input_feature_dim, num_experts, hidden_dims=None, dropout_rate=0.0): super(GatingNetwork, self).__init__() layers = [] last_dim = input_feature_dim # If hidden layers are specified, create them if hidden_dims is not None: for hidden_dim in hidden_dims: layers.append(nn.Linear(last_dim, hidden_dim)) layers.append(nn.ReLU()) # You could make this a hyperparameter as well if dropout_rate &gt; 0.0: layers.append(nn.Dropout(dropout_rate)) last_dim = hidden_dim # Final layer projecting to the number of experts layers.append(nn.Linear(last_dim, num_experts)) self.fc_layers = nn.Sequential(*layers) self.softmax = nn.Softmax(dim=1) def forward(self, x): # Assuming x is of shape [batch_size, seq_len, d_model], aggregate across the sequence length x = x.mean(dim=1) # Aggregate feature per instance x = self.fc_layers(x) # Pass through gating network layers return self.softmax(x) # Define hyperparameters specific to the transformer d_model = 384 #128 nhead = 8 #8 dim_feedforward = 768 #256 num_encoder_layers = 8 #2 num_experts = 2 #2 hidden_dims = [512, 256] # List of hidden layer sizes dropout_rate = 0.5 # Dropout rate gating_network = GatingNetwork( input_feature_dim=d_model, num_experts=num_experts, hidden_dims=hidden_dims, dropout_rate=dropout_rate, ) # Mixture of Experts Model class MixtureOfTransformerExperts(nn.Module): def __init__(self, input_size, d_model, output_size, nhead, dim_feedforward, num_experts, num_encoder_layers=1): super(MixtureOfTransformerExperts, self).__init__() self.num_experts = num_experts self.output_size = output_size # Store output_size as an instance variable self.experts = nn.ModuleList([TransformerExpert(input_size, d_model, output_size, nhead, dim_feedforward, num_encoder_layers) for _ in range(num_experts)]) self.gating_network = GatingNetwork(d_model, num_experts) def forward(self, x): gating_scores = self.gating_network(x) # [batch_size, num_experts] expert_outputs = [expert(x) for expert in self.experts] # List of [batch_size, seq_len, output_size] stacked_expert_outputs = torch.stack(expert_outputs) # Shape: [num_experts, batch_size, seq_len, output_size] # Expand gating scores expanded_gating_scores = gating_scores.unsqueeze(2).unsqueeze(3) # Shape: [batch_size, num_experts, 1, 1] expanded_gating_scores = expanded_gating_scores.expand(-1, -1, x.size(1), self.output_size) expanded_gating_scores = expanded_gating_scores.transpose(0, 1) # Shape: [num_experts, batch_size, seq_len, output_size] # Now the shape of expanded_gating_scores matches stacked_expert_outputs, and broadcasting will work mixed_output = torch.sum(stacked_expert_outputs * expanded_gating_scores, dim=0) # Sum weighted expert outputs for each time step return mixed_output class QAJsonlDataset(Dataset): def __init__(self, path, seq_len): self.seq_len = seq_len self.pairs = self.load_data(path) # Flatten the pairs completely before passing them to build_vocab self.vocab, self.idx2token = self.build_vocab([word for pair in self.pairs for sublist in pair for word in sublist]) self.tokenized_pairs = [(self.tokenize(q), self.tokenize(a)) for q, a in self.pairs] def load_data(self, path): pairs = [] with open(path, &quot;r&quot;, encoding=&quot;utf-8&quot;) as f: for line in f: data = json.loads(line.strip()) question, answer = data.get(&quot;question&quot;, &quot;&quot;), data.get(&quot;answer&quot;, &quot;&quot;) pairs.append((question.split(), answer.split())) return pairs def tokenize(self, words): # Tokenize a sentence and pad if necessary # Add &lt;eos&gt; token at the end if theres room tokens = [self.vocab.get(w, self.vocab[&quot;&lt;unk&gt;&quot;]) for w in words] if len(tokens) &lt; self.seq_len: tokens.append(self.vocab[&quot;&lt;eos&gt;&quot;]) # Add &lt;eos&gt; token tokens.extend([self.vocab[&quot;&lt;pad&gt;&quot;]] * (self.seq_len - len(tokens))) # Pad the rest else: tokens = tokens[:self.seq_len - 1] + [self.vocab[&quot;&lt;eos&gt;&quot;]] return tokens def build_vocab(self, words): # Start with special tokens with fixed indices vocab = {&quot;&lt;unk&gt;&quot;: 0, &quot;&lt;pad&gt;&quot;: 1, &quot;&lt;eos&gt;&quot;: 2} start_index = len(vocab) # Use Counter to count word frequencies in the corpus counts = Counter(words) # Create the vocab dictionary with all words, starting indices after the special tokens for word, _ in counts.most_common(): if word not in vocab: # Skip special tokens vocab[word] = len(vocab) # Create the reverse mapping from indices to words idx2token = {idx: token for token, idx in vocab.items()} return vocab, idx2token def __len__(self): return len(self.tokenized_pairs) def __getitem__(self, idx): tokenized_question, tokenized_answer = self.tokenized_pairs[idx] return torch.tensor(tokenized_question, dtype=torch.long), torch.tensor(tokenized_answer, dtype=torch.long) class MoETransformerModel(nn.Module): def __init__(self, vocab_size, d_model, moe): super(MoETransformerModel, self).__init__() self.embedding = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model) self.moe = moe self.dropout = nn.Dropout(p=0.125) # Dropout added for regularization def forward(self, x): embedded = self.dropout(self.embedding(x)) return self.moe(embedded) # Remove positional encoding addition here, as its already added in TransformerExpert def collate_fn(batch): questions, answers = zip(*batch) questions = pad_sequence(questions, batch_first=True, padding_value=0) answers = pad_sequence(answers, batch_first=True, padding_value=0) return questions, answers # Set the path to your text file and define sequence length path_to_text = &#39;train.jsonl&#39; # replace with the path to your text file seq_len = 24 # sequence length # Create a dataset and data loader dataset = QAJsonlDataset(path_to_text, seq_len) data_loader = DataLoader(dataset, batch_size=24, shuffle=True, collate_fn=collate_fn, pin_memory=True) # Training loop - added gradient clipping to avoid exploding gradients def train_model(model, criterion, optimizer, num_epochs, data_loader): model.train() for epoch in range(num_epochs): total_loss = 0 progress_bar = tqdm(enumerate(data_loader), total=len(data_loader), desc=f&quot;Epoch {epoch+1}&quot;, leave=False) for i, (inputs, targets) in progress_bar: inputs, targets = inputs.to(device), targets.to(device) optimizer.zero_grad() predictions = model(inputs) predictions = predictions.view(-1, predictions.size(-1)) targets = targets.view(-1) # Flatten the targets to [batch_size * seq_len] loss = criterion(predictions, targets) loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0) # Gradient clipping optimizer.step() total_loss += loss.item() progress_bar.set_postfix({&quot;Loss&quot;: loss.item()}) average_loss = total_loss / len(data_loader.dataset) print(f&quot;Epoch {epoch+1}, Average Loss: {average_loss}&quot;) def generate_text(model, dataset, seed_text, num_generate, temperature=1.0): model.eval() # Put the model in evaluation mode # List to store the generated tokens generated_tokens = [] # Initial sequence (prefix) to start the generation process input_sequence = [dataset.vocab.get(word, dataset.vocab[&quot;&lt;pad&gt;&quot;]) for word in seed_text.split()] # Convert to token IDs current_sequence = torch.tensor(input_sequence, dtype=torch.long).unsqueeze(0) current_sequence = current_sequence.to(device) # Generate num_generate tokens for _ in range(num_generate): # Forward pass through the model with torch.no_grad(): output = model(current_sequence) # Get probabilities, apply temperature scaling, and sample from the distribution probabilities = F.softmax(output[:, -1, :] / temperature, dim=-1).detach() next_token_idx = torch.multinomial(probabilities, 1).item() # Append token to the current sequence and to the generated tokens generated_tokens.append(next_token_idx) current_sequence = torch.cat((current_sequence, torch.tensor([[next_token_idx]])), 1).to(device) # Convert tokens to words generated_text = &quot; &quot;.join([dataset.idx2token.get(token, &quot;&lt;unk&gt;&quot;) for token in generated_tokens]) # Use .get() to provide a default value for missing keys return generated_text # Function to count the number of tokens in the dataset def count_tokens_in_dataset(dataset): return sum([len(pair[0]) + len(pair[1]) for pair in dataset.pairs]) num_tokens = count_tokens_in_dataset(dataset) print(f&quot;Total number of tokens in the dataset: {num_tokens}&quot;) vocab_size = len(dataset.vocab) # Assume dataset.vocab is defined in the QAJsonlDataset class # Instantiate resulting MoE transformer model and move it to device moe = MixtureOfTransformerExperts( input_size=d_model, d_model=d_model, output_size=vocab_size, nhead=nhead, dim_feedforward=dim_feedforward, num_experts=num_experts, num_encoder_layers=num_encoder_layers ).to(device) # Instantiate the MoE transformer model and move it to device moe_transformer_model = MoETransformerModel(vocab_size, d_model, moe).to(device) def count_parameters(model): return sum(p.numel() for p in model.parameters() if p.requires_grad) # Example usage with your model: total_params = count_parameters(moe_transformer_model) print(f&quot;Total trainable parameters: {total_params}&quot;) # Training parameters num_epochs = 2 learning_rate = 0.00045 # Define Loss Function and Optimizer for MoE model - using Label Smoothing for better generalization criterion = nn.CrossEntropyLoss(label_smoothing=0.1) optimizer = torch.optim.AdamW(moe_transformer_model.parameters(), lr=learning_rate, weight_decay=0.01) # Using AdamW with weight decay # Train the model with the text data train_model(moe_transformer_model, criterion, optimizer, num_epochs, data_loader) # Start a loop for the interactive chat-like text generation while True: try: # Get user input seed_text = input(&quot;Enter seed text (type &#39;quit&#39; to stop): &quot;) # Check if user wants to quit the interaction if seed_text.lower() == &quot;quit&quot;: print(&quot;Exiting text generation chat.&quot;) break # User input is not empty and not quit, generate text if seed_text.strip(): num_generate = 20 # Number of words to generate temperature = 1.0 # Sampling temperature, higher will increase diversity # Use the trained model to generate text generated_text = generate_text(moe_transformer_model, dataset, seed_text, num_generate, temperature) print(&quot;Generated Text:&quot;, generated_text) else: print(&quot;Seed text cannot be empty.&quot;) except KeyboardInterrupt: # Handle KeyboardInterrupt (Ctrl+C) to gracefully exit print(&quot;nExiting text generation chat.&quot;) break torch.save(moe_transformer_model.state_dict(), &quot;MoE_Transformer-X4-78M-QA.pth&quot;)</div><div class="divider" data-v-9c550d47></div><p class="review-content" data-v-9c550d47><span class="icon" data-v-9c550d47>answer:</span>To modify the code to save the vocabulary and the configuration of the model into appropriate files after the model has been instantiated but before the training starts, you can add a few lines of code. Here&#39;s how you can do it: 1. Save the vocabulary to a text file. 2. Save the configuration of the model to a JSON file. Here&#39;s the modified code with the relevant parts highlighted: ```python # Existing code (up until MixtureOfTransformerExperts class)... # Create a dataset and data loader dataset = QAJsonlDataset(path_to_text, seq_len) # Save vocabulary to a text file vocab_file = &quot;vocabulary.txt&quot; with open(vocab_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: for token, id in dataset.vocab.items(): f.write(f&quot;{token}t{id}n&quot;) # Model configuration parameters to be saved model_config = { &quot;d_model&quot;: d_model, &quot;nhead&quot;: nhead, &quot;dim_feedforward&quot;: dim_feedforward, &quot;num_encoder_layers&quot;: num_encoder_layers, &quot;num_experts&quot;: num_experts, &quot;hidden_dims&quot;: hidden_dims, &quot;dropout_rate&quot;: dropout_rate } # Save configuration to a JSON file config_file = &quot;model_config.json&quot; with open(config_file, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f: json.dump(model_config, f, indent=4) # Continue with the remaining code... data_loader = DataLoader(dataset, batch_size=24, shuffle=True, collate_fn=collate_fn, pin_memory=True) # Instantiate resulting MoE transformer model and move it to device... # Rest of the code follows as is, such as training, interactive text generation, etc. ``` With the above modifications, after instantiating your dataset, we save the vocabulary to a text file by iterating over the `vocab` dictionary and writing each word and its index as tab-separated values on separate lines. For the model configuration, we save the relevant parameters that define the model structure to a JSON file. This configuration file can later be used to instantiate the same model structure with the saved parameters. Remember to save these files before starting the model training, especially if you&#39;re considering distributing the model, as having access to the vocabulary and model configuration is crucial for users to correctly load and use the trained model. Lastly, you should also ensure that your script has the appropriate permissions and file paths are specified to save files to disk.</p></div></div><div class="poem-container" data-v-2262462c data-v-9c550d47><div class="review" data-v-9c550d47><div class="review-title" data-v-9c550d47><span class="icon" data-v-9c550d47>question:</span>complete this and return complete code: import torch import torch.nn as nn from torch.nn.functional import relu from torch.utils.data import Dataset, DataLoader # Utility functions def positional_encoding(seq_len, d_model): pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) pe = torch.zeros(seq_len, d_model) pe[:, 0::2] = torch.sin(pos * div_term) pe[:, 1::2] = torch.cos(pos * div_term) return pe.unsqueeze(0) class TextDataset(Dataset): def __init__(self, text_file_path, seq_len): self.seq_len = seq_len with open(text_file_path, &quot;r&quot;) as f: self.text = f.read().split() def __len__(self): return len(self.text) // self.seq_len def __getitem__(self, idx): start_idx = idx * self.seq_len end_idx = min(start_idx + self.seq_len, len(self.text)) tokens = self.text[start_idx:end_idx] return torch.tensor([self.vocab.get(token, self.vocab[&quot;&lt;unk&gt;&quot;]) for token in tokens]) # Model definition class MambaBlock(nn.Module): def __init__(self, d_model, nhead, dim_feedforward): super().__init__() self.multihead_attn = nn.MultiheadAttention(d_model, nhead) self.ffn = nn.Sequential( nn.Linear(d_model, dim_feedforward), relu(), nn.Linear(dim_feedforward, d_model) ) self.layer_norm1 = nn.LayerNorm(d_model) self.layer_norm2 = nn.LayerNorm(d_model) def forward(self, x): # Self-attention attn_output = self.multihead_attn(x, x, x) x = x + attn_output x = self.layer_norm1(x) # Feed-forward network ffn_output = self.ffn(x) x = x + ffn_output x = self.layer_norm2(x) return x class MambaModel(nn.Module): def __init__(self, input_dim, d_model, nhead, dim_feedforward, num_blocks, output_dim): super().__init__() self.embedding = nn.Embedding(input_dim, d_model) self.positional_encoding = positional_encoding(seq_len, d_model) self.blocks = nn.ModuleList([MambaBlock(d_model, nhead, dim_feedforward) for _ in range(num_blocks)]) self.output_layer = nn.Linear(d_model, output_dim) def forward(self, x): # Embed tokens embedded_x = self.embedding(x) # Add positional encoding embedded_x = embedded_x + self.positional_encoding # Pass through Mamba blocks for block in self.blocks: embedded_x = block(embedded_x) # Apply output layer output = self.output_layer(embedded_x) return output # Training loop def train_model(model, train_loader, criterion, optimizer, num_epochs): for epoch in range(num_epochs): for batch in train_loader: # Clear gradients optimizer.zero_grad() # Forward pass outputs = model(batch) # Calculate loss loss = criterion(outputs, targets) # Replace targets with appropriate labels # Backward pass and optimization loss.backward() optimizer.step() # Print progress print(f&quot;Epoch: {epoch+1}/{num_epochs}, Loss: {loss.f}&quot;) # Test with human input def test_with_human_input(): prompt = input(&quot;Enter a text prompt: &quot;) output = model.generate(prompt) # Use appropriate generation method print(f&quot;Generated text: {output}&quot;) feedback = input(&quot;Is the generated text relevant and coherent? (y/n): &quot;) # Store feedback (e.g., in a separate file) for analysis # Example usage train_model(...) # Train the model test_with_human_input()</div><div class="divider" data-v-9c550d47></div><p class="review-content" data-v-9c550d47><span class="icon" data-v-9c550d47>answer:</span>Below is the completed code with the necessary parts filled in, including a functioning training loop, a method to load and process the dataset, and a simple human input test mechanism. Note that it&#39;s not immediately ready to run without a dataset and requires further implementation details, such as the output dimension, the target data during training, the generation method, etc. ```python import torch import torch.nn as nn from torch.nn.functional import relu from torch.utils.data import Dataset, DataLoader import math # Utility functions def positional_encoding(seq_len, d_model): pos = torch.arange(0, seq_len, dtype=torch.float).unsqueeze(1) div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)) pe = torch.zeros(seq_len, d_model) pe[:, 0::2] = torch.sin(pos * div_term) pe[:, 1::2] = torch.cos(pos * div_term) return pe.unsqueeze(0) # TextDataset previous definition # Model definition as before: MambaBlock, MambaModel # Assume that input_dim and output_dim are defined as per the application&#39;s requirements # input_dim = size of the vocabulary # output_dim = size of the output vocabulary (if different) or same as input_dim # Other necessary parameters seq_len = 50 # Example sequence length d_model = 512 # Model dimension nhead = 8 # Number of attention heads dim_feedforward = 2048 # Dimension of the feedforward layer num_blocks = 6 # Number of transformer blocks # Instantiate model model = MambaModel(input_dim, d_model, nhead, dim_feedforward, num_blocks, output_dim) # Training loop with targets and criterion defined def train_model(model, train_loader, criterion, optimizer, num_epochs): model.train() for epoch in range(num_epochs): for batch in train_loader: inputs, targets = batch # Assumes batch contains a tuple (inputs, targets) # Clear gradients optimizer.zero_grad() # Forward pass outputs = model(inputs) # Calculate loss loss = criterion(outputs.view(-1, outputs.size(-1)), targets.view(-1)) # Backward pass and optimization loss.backward() optimizer.step() # Print progress print(f&quot;Epoch: {epoch+1}/{num_epochs}, Loss: {loss.item()}&quot;) # Test with human input def test_with_human_input(model, vocab, idx2token): model.eval() # Set the model to evaluation mode prompt = input(&quot;Enter a text prompt: &quot;) # Generate output based on the prompt # The generation method is not implemented since it can vary greatly depending on the task # This is just a placeholder for the completion of the process output = model.generate(prompt) # You&#39;ll need to create the generate method print(f&quot;Generated text: {output}&quot;) feedback = input(&quot;Is the generated text relevant and coherent? (y/n): &quot;) # Store feedback (e.g., in a separate file) for analysis # Example usage # Define criterion and optimizer criterion = nn.CrossEntropyLoss() # Assuming a classification task optimizer = torch.optim.Adam(model.parameters(), lr=0.001) # Dataset and DataLoader instantiation would go here # ... # Train the model with the dataset num_epochs = 10 train_loader = DataLoader(...) # Properly defined DataLoader train_model(model, train_loader, criterion, optimizer, num_epochs) # Test trained model with human input test_with_human_input(model, vocab, idx2token) ``` In the training loop, the targets tensor has been added, and it&#39;s assumed to be part of the provided DataLoader batches. The `nn.CrossEntropyLoss` is utilized as the loss function, which expects the model outputs to be shaped as `[batch_size*seq_len, output_dim]` and the targets to be shaped as `[batch_size*seq_len]` when calculating the loss. Therefore, the outputs and targets are reshaped accordingly before calling the loss function. For the human input test, a function `test_with_human_input` with `model`, `vocab`, and `idx2token` as arguments is defined to interact with the user, but without implementing a specific generation method. You&#39;ll need to create this generation method based on the task at hand (e.g., text completion, language modeling, etc.). The `model.generate` method placeholder is meant to represent a custom implementation in `MambaModel`, which your specific application needs (e.g., using beam search or greedy sampling). The `vocab` and `idx2token` are placeholders and need to be defined based on your particular vocabulary setup. This completed code aims to show how various parts of a PyTorch model and training procedure can be structured, but the actual implementation details are application-specific and thus require additional completion of these parts for a functioning model.</p></div></div><!--]--><!----></div><button class="load-more-button" data-v-2262462c><span data-v-2262462c>See more</span></button></section></div></div></main><!--[--><!--]--><div class="VPFooter" data-v-b0366457><p class="license" data-v-b0366457>Released under the <a class="vt-link link link" href="https://www.m44m.com" target="_blank" rel="noopener noreferrer" data-v-b0366457><!--[-->stanford License<!--]--><!----><!----></a>.</p><p class="copyright" data-v-b0366457>Copyright  2014-2025 stanford</p></div><!--[--><!--]--></div></div><div class="visually-hidden" aria-live="polite" data-v-e4982c5a> has loaded</div></div></div>
    <script>window.__VP_HASH_MAP__=JSON.parse("{\"about_coc.md\":\"B3ITOP5z\",\"about_community-guide.md\":\"Dvcdb1O6\",\"about_faq.md\":\"20McVa9n\",\"about_privacy.md\":\"CcWPOTgs\",\"about_releases.md\":\"GqqAVeGY\",\"about_team.md\":\"Bnw40y2b\",\"chatai_1.md\":\"Bj_FCoRo\",\"chatai_10.md\":\"Bwrk5B4R\",\"chatai_11.md\":\"eQLJ080f\",\"chatai_12.md\":\"BQY3M2uj\",\"chatai_13.md\":\"uo7f8oNr\",\"chatai_14.md\":\"BLXHWiOp\",\"chatai_15.md\":\"nQdxXbI5\",\"chatai_16.md\":\"VrJ_5zSi\",\"chatai_17.md\":\"DUm2k3DO\",\"chatai_18.md\":\"CqRE6jSd\",\"chatai_19.md\":\"BdrGr_wp\",\"chatai_2.md\":\"B_qgG-NX\",\"chatai_20.md\":\"CEyscjW3\",\"chatai_21.md\":\"DAWBEETu\",\"chatai_22.md\":\"Bi4GIhOA\",\"chatai_23.md\":\"Chv0-Ztb\",\"chatai_24.md\":\"BqVPC9n0\",\"chatai_25.md\":\"DQvkZ7pr\",\"chatai_26.md\":\"DAT3dMEm\",\"chatai_27.md\":\"DX_yxMT8\",\"chatai_28.md\":\"BwEwP6Rg\",\"chatai_29.md\":\"D7lc77eg\",\"chatai_3.md\":\"Bx-OoA1v\",\"chatai_30.md\":\"jreHahpA\",\"chatai_31.md\":\"DUgyZgf6\",\"chatai_32.md\":\"DA8sT6kL\",\"chatai_33.md\":\"DPoucB3K\",\"chatai_34.md\":\"33lUq0WB\",\"chatai_35.md\":\"DoZXtJoR\",\"chatai_36.md\":\"CtlbrAxo\",\"chatai_37.md\":\"NUZqkLtj\",\"chatai_38.md\":\"DfnKQb7k\",\"chatai_39.md\":\"Dgsl2tKZ\",\"chatai_4.md\":\"mjzdQuV1\",\"chatai_40.md\":\"BXwlFXqB\",\"chatai_41.md\":\"BRjgI3Co\",\"chatai_42.md\":\"OOR6FjH7\",\"chatai_43.md\":\"hYOEtvGL\",\"chatai_44.md\":\"BYK4BZ2a\",\"chatai_45.md\":\"BVVhaOYT\",\"chatai_46.md\":\"DS2_1if_\",\"chatai_47.md\":\"CbBnd1-r\",\"chatai_48.md\":\"Bp1ImqYI\",\"chatai_49.md\":\"fGO6OsYU\",\"chatai_5.md\":\"CWOy8EmI\",\"chatai_50.md\":\"Bb4NsgH1\",\"chatai_51.md\":\"CYOAzjQx\",\"chatai_52.md\":\"CagMQhi8\",\"chatai_53.md\":\"eJvwiYKO\",\"chatai_54.md\":\"Ci8xYB5J\",\"chatai_55.md\":\"Ck_YytfR\",\"chatai_56.md\":\"DHIdJcyW\",\"chatai_57.md\":\"CuInSyfP\",\"chatai_58.md\":\"Bs4sdJ15\",\"chatai_59.md\":\"Bzqwllj5\",\"chatai_6.md\":\"9kit50QA\",\"chatai_60.md\":\"ARaLlRW-\",\"chatai_61.md\":\"CwXtVQNL\",\"chatai_62.md\":\"N5dWr__z\",\"chatai_63.md\":\"3JYEI8jl\",\"chatai_64.md\":\"DwhMaoMU\",\"chatai_65.md\":\"BMUDxbwz\",\"chatai_66.md\":\"CK-A415L\",\"chatai_67.md\":\"D6dNXYu9\",\"chatai_68.md\":\"D8-j70Ri\",\"chatai_7.md\":\"CGuruNIR\",\"chatai_8.md\":\"YazK29fm\",\"chatai_9.md\":\"CnBIFfrD\",\"chatai_index.md\":\"hL1i_FIH\",\"deepseek_1.md\":\"DJm_8bxn\",\"deepseek_10.md\":\"CoSdElCK\",\"deepseek_11.md\":\"CoWsx2xV\",\"deepseek_12.md\":\"CwkBsurI\",\"deepseek_13.md\":\"DM-ctXja\",\"deepseek_14.md\":\"Cf5_apzl\",\"deepseek_15.md\":\"DSZvnFyO\",\"deepseek_16.md\":\"BcouKnDv\",\"deepseek_17.md\":\"ChlWHwtv\",\"deepseek_18.md\":\"BQeOi0YE\",\"deepseek_19.md\":\"ByHgvnZy\",\"deepseek_2.md\":\"DVjNBdN-\",\"deepseek_20.md\":\"Di0XzJVV\",\"deepseek_21.md\":\"DvANmLBe\",\"deepseek_22.md\":\"BGkMvZ6W\",\"deepseek_23.md\":\"CeHdJqKn\",\"deepseek_24.md\":\"Bb4HXM-o\",\"deepseek_25.md\":\"BrRaILs2\",\"deepseek_26.md\":\"CNoqZp36\",\"deepseek_27.md\":\"ImJZFkE3\",\"deepseek_28.md\":\"CcmTWtx5\",\"deepseek_29.md\":\"DOIGevfY\",\"deepseek_3.md\":\"ruquTUgc\",\"deepseek_30.md\":\"BrJaNs8r\",\"deepseek_31.md\":\"CfaMK7Jl\",\"deepseek_32.md\":\"CtMiPqRg\",\"deepseek_33.md\":\"CYfV7Mxp\",\"deepseek_34.md\":\"ptd06baQ\",\"deepseek_35.md\":\"Boj6iQX4\",\"deepseek_36.md\":\"DAb_Zf5V\",\"deepseek_37.md\":\"nT6WTmH3\",\"deepseek_38.md\":\"cbEm6v_f\",\"deepseek_39.md\":\"AvT3fg6a\",\"deepseek_4.md\":\"CAYHL252\",\"deepseek_40.md\":\"D471dkV8\",\"deepseek_41.md\":\"Bbqf7f5K\",\"deepseek_42.md\":\"8Q9jQXxX\",\"deepseek_43.md\":\"BKqRU0WP\",\"deepseek_44.md\":\"Ca3yqTwN\",\"deepseek_45.md\":\"CbgSCjA5\",\"deepseek_46.md\":\"CO6RJaoZ\",\"deepseek_47.md\":\"BMLjyUBv\",\"deepseek_48.md\":\"CVt4-eIS\",\"deepseek_49.md\":\"T1UaZR99\",\"deepseek_5.md\":\"CQh8gmDT\",\"deepseek_50.md\":\"BTdpmGQa\",\"deepseek_51.md\":\"Dpxor1-C\",\"deepseek_52.md\":\"XOYH8lsn\",\"deepseek_53.md\":\"IjZXP440\",\"deepseek_54.md\":\"IQe96A_X\",\"deepseek_55.md\":\"B_6YjNTb\",\"deepseek_56.md\":\"NA4E2dK6\",\"deepseek_57.md\":\"BKDhmgd1\",\"deepseek_58.md\":\"HD_ah8R7\",\"deepseek_59.md\":\"_t8FA5so\",\"deepseek_6.md\":\"DI3T00GH\",\"deepseek_60.md\":\"DkvyJsrq\",\"deepseek_61.md\":\"DsYVMEfs\",\"deepseek_62.md\":\"DX8Z1nG4\",\"deepseek_63.md\":\"B1ctzRnZ\",\"deepseek_64.md\":\"L20UdQgq\",\"deepseek_65.md\":\"n--OdS48\",\"deepseek_66.md\":\"0EU7duaJ\",\"deepseek_67.md\":\"C2MI6_D1\",\"deepseek_68.md\":\"BrQRsoeC\",\"deepseek_7.md\":\"DNMURS_3\",\"deepseek_8.md\":\"D70RZa38\",\"deepseek_9.md\":\"C81_YJnh\",\"drive_1.md\":\"BZ8GGuSs\",\"drive_10.md\":\"BxKjQaik\",\"drive_11.md\":\"DAcoKqRL\",\"drive_12.md\":\"DEVxq_Iv\",\"drive_13.md\":\"5fpPZ6J2\",\"drive_14.md\":\"Bbkb-Bc1\",\"drive_15.md\":\"CAB8_jF3\",\"drive_16.md\":\"B5uNfJM4\",\"drive_17.md\":\"BcWm8Res\",\"drive_18.md\":\"CyMhipMF\",\"drive_19.md\":\"DitdxNVi\",\"drive_2.md\":\"TgJQaxLg\",\"drive_20.md\":\"rWHCnPXW\",\"drive_21.md\":\"0EuUKtud\",\"drive_22.md\":\"nlLzemCF\",\"drive_23.md\":\"Bl2_buHh\",\"drive_24.md\":\"CTZsSgRn\",\"drive_25.md\":\"BYS07TQu\",\"drive_26.md\":\"BhCS2Ti7\",\"drive_27.md\":\"lRsmn0Lk\",\"drive_28.md\":\"utvNfchl\",\"drive_29.md\":\"DxtP6OnG\",\"drive_3.md\":\"CbezJhYk\",\"drive_30.md\":\"BWIpCldh\",\"drive_31.md\":\"wj7EYlkt\",\"drive_32.md\":\"Bxp2IY7d\",\"drive_33.md\":\"BG7YqHVK\",\"drive_34.md\":\"Svf60gDt\",\"drive_35.md\":\"DuqLQYDP\",\"drive_36.md\":\"VHSGLPdw\",\"drive_37.md\":\"Ddr3jlob\",\"drive_38.md\":\"BCnPebk8\",\"drive_39.md\":\"GyNZnWkd\",\"drive_4.md\":\"D53e5hiu\",\"drive_40.md\":\"CnvPE75K\",\"drive_41.md\":\"DIJJcQC0\",\"drive_42.md\":\"BvBjx9tq\",\"drive_43.md\":\"D2QNlWMC\",\"drive_44.md\":\"CT9T4K75\",\"drive_45.md\":\"C_zOeNcY\",\"drive_46.md\":\"xlusuZ1E\",\"drive_47.md\":\"BB4xOo7m\",\"drive_48.md\":\"rG7oTX7L\",\"drive_49.md\":\"D9FUV6nl\",\"drive_5.md\":\"Bg0YgWe_\",\"drive_50.md\":\"BJ-YJuUs\",\"drive_51.md\":\"9JF0ZG43\",\"drive_52.md\":\"VbhWsv47\",\"drive_53.md\":\"BZmIOb6e\",\"drive_54.md\":\"TvZUJCvz\",\"drive_55.md\":\"BVxJFvmn\",\"drive_56.md\":\"BRcrM4F5\",\"drive_57.md\":\"Dx-20o93\",\"drive_58.md\":\"DRmyJaMJ\",\"drive_59.md\":\"CzdklufH\",\"drive_6.md\":\"BNHahSx-\",\"drive_60.md\":\"Gsr9YCjg\",\"drive_7.md\":\"BouMitdh\",\"drive_8.md\":\"K0qRiFZF\",\"drive_9.md\":\"DPYVNJcj\",\"drive_aiprompt.md\":\"BB41XtLC\",\"drive_deep.md\":\"BnPdsWJC\",\"drive_donation.md\":\"8wvNJAVf\",\"drive_gpt.md\":\"DtNf00o-\",\"drive_prompt.md\":\"BoXlmEzc\",\"drive_promptlibrary.md\":\"D2zvzxSB\",\"drive_team.md\":\"CkNmWMDG\",\"ecosystem_chatgpt.md\":\"CecoSlph\",\"ecosystem_deepseek.md\":\"CF1UvxTr\",\"ecosystem_navigation.md\":\"wKlND_1J\",\"ecosystem_newsletters.md\":\"DaQl6Mbh\",\"ecosystem_projects.md\":\"C3EFuYoi\",\"ecosystem_promptes.md\":\"DZRC8AXP\",\"ecosystem_themes.md\":\"DcxYtm4j\",\"error-reference_index.md\":\"C8cWCSv1\",\"examples_index.md\":\"DYIYBDwT\",\"grok_1.md\":\"ZRUiyo8p\",\"grok_10.md\":\"D7RAczjB\",\"grok_11.md\":\"Dn7kknDg\",\"grok_12.md\":\"BNvC5Z3f\",\"grok_13.md\":\"D4Zg3VYg\",\"grok_14.md\":\"BWUnF1tH\",\"grok_15.md\":\"BqZ3bLR1\",\"grok_16.md\":\"n6AbpC-3\",\"grok_17.md\":\"D3PhxGc2\",\"grok_18.md\":\"cEFDB3VM\",\"grok_19.md\":\"BdEaC4iv\",\"grok_2.md\":\"-upizpES\",\"grok_20.md\":\"CrsYtaeR\",\"grok_21.md\":\"DjbWKE1Q\",\"grok_22.md\":\"BYO85xfs\",\"grok_23.md\":\"BuJv06Zv\",\"grok_24.md\":\"BlkxfHTQ\",\"grok_25.md\":\"GOANqNts\",\"grok_26.md\":\"BLsiJjEE\",\"grok_27.md\":\"B5DKGARG\",\"grok_28.md\":\"BT3x7Z3B\",\"grok_29.md\":\"DmI8zstM\",\"grok_3.md\":\"D5rDnONU\",\"grok_30.md\":\"CPEKouv6\",\"grok_31.md\":\"CQdRLAcK\",\"grok_32.md\":\"CXQNGZTC\",\"grok_33.md\":\"DbZcZond\",\"grok_34.md\":\"DU12t5Ez\",\"grok_35.md\":\"BME-UaiO\",\"grok_36.md\":\"DrKiqPRv\",\"grok_37.md\":\"CnW_kitA\",\"grok_38.md\":\"DY-VGDq5\",\"grok_39.md\":\"TOZEBVUM\",\"grok_4.md\":\"ueMlqdFc\",\"grok_40.md\":\"CHJMXLDS\",\"grok_41.md\":\"CXkRSeAu\",\"grok_42.md\":\"B0EIfpd4\",\"grok_43.md\":\"DEoUwLsO\",\"grok_44.md\":\"BXUYFa1X\",\"grok_45.md\":\"CxbNwViZ\",\"grok_46.md\":\"DUaYNjdK\",\"grok_47.md\":\"DDtYIYcx\",\"grok_48.md\":\"8AbwN0dr\",\"grok_49.md\":\"BLWXG_Mj\",\"grok_5.md\":\"usCrDKBd\",\"grok_50.md\":\"BOXnvAng\",\"grok_51.md\":\"CBCxxT8l\",\"grok_52.md\":\"BTlUSP-o\",\"grok_53.md\":\"qVmY7mye\",\"grok_54.md\":\"0DI21AKa\",\"grok_55.md\":\"2lfyl6I8\",\"grok_56.md\":\"CA1duLSZ\",\"grok_57.md\":\"-ZUanYtY\",\"grok_58.md\":\"C5f1hzrZ\",\"grok_59.md\":\"oUzyMbih\",\"grok_6.md\":\"BWk1Sxjl\",\"grok_60.md\":\"BZdw6cxk\",\"grok_61.md\":\"C6MiQxuy\",\"grok_62.md\":\"DGNpH81I\",\"grok_63.md\":\"DHPkaP5y\",\"grok_64.md\":\"BHJI0qpc\",\"grok_65.md\":\"Cu_7X8qK\",\"grok_66.md\":\"DIteB7qK\",\"grok_67.md\":\"DQp6fnXA\",\"grok_68.md\":\"D83u-o42\",\"grok_7.md\":\"DsA9KOaF\",\"grok_8.md\":\"BykplNg8\",\"grok_9.md\":\"B0vutZzU\",\"guide_1.md\":\"BjT7Eork\",\"guide_10.md\":\"8jtaVdcl\",\"guide_11.md\":\"iAXBbNcP\",\"guide_12.md\":\"k_ETsDVB\",\"guide_13.md\":\"9cWKv5xE\",\"guide_14.md\":\"BrmHyrSk\",\"guide_15.md\":\"B-0RDC7q\",\"guide_16.md\":\"DfsrxIzZ\",\"guide_17.md\":\"l3wpOSk1\",\"guide_18.md\":\"5BItISi8\",\"guide_19.md\":\"DuKAVqQm\",\"guide_2.md\":\"VWBmnxFf\",\"guide_20.md\":\"VbldHV-w\",\"guide_21.md\":\"CQvw7J1n\",\"guide_22.md\":\"ClWWi2Q7\",\"guide_23.md\":\"C5DkoZ8V\",\"guide_24.md\":\"Bu_5d6Kv\",\"guide_25.md\":\"LRaTBO-I\",\"guide_26.md\":\"CBJiJTB7\",\"guide_27.md\":\"80Cfke_Y\",\"guide_28.md\":\"BW_ioSN_\",\"guide_29.md\":\"Cr3C2d6n\",\"guide_3.md\":\"CU-s4sAf\",\"guide_30.md\":\"BmOOQMxm\",\"guide_31.md\":\"D-oMDyM3\",\"guide_32.md\":\"Dca3GfB0\",\"guide_33.md\":\"DvqpbkI7\",\"guide_34.md\":\"Dn8qvDAz\",\"guide_35.md\":\"DDyGYCu4\",\"guide_36.md\":\"DRN0TB9o\",\"guide_37.md\":\"D10Ld-qj\",\"guide_38.md\":\"Dl-u7Rjl\",\"guide_39.md\":\"D4zO3mEq\",\"guide_4.md\":\"DzYRrNfC\",\"guide_40.md\":\"Dk6M_wLW\",\"guide_41.md\":\"DDOfYWkB\",\"guide_42.md\":\"B5MgxCoQ\",\"guide_43.md\":\"se5u3JoO\",\"guide_44.md\":\"_bC_mwfx\",\"guide_45.md\":\"BlN_O5hX\",\"guide_46.md\":\"BWlsZ83c\",\"guide_47.md\":\"jn-19LRF\",\"guide_48.md\":\"CWs3E0N7\",\"guide_49.md\":\"1q7Tl8gL\",\"guide_5.md\":\"B_A444kA\",\"guide_50.md\":\"BTO3E3Jx\",\"guide_51.md\":\"DS7NUv9H\",\"guide_52.md\":\"G8s8iwFM\",\"guide_53.md\":\"C-StcWtH\",\"guide_54.md\":\"Bpb9fQPf\",\"guide_55.md\":\"CeBsbOuZ\",\"guide_56.md\":\"CYQCBi0v\",\"guide_57.md\":\"B8i2riVb\",\"guide_58.md\":\"DjR8N2kI\",\"guide_59.md\":\"CxJW_Glq\",\"guide_6.md\":\"D8do4Y6Z\",\"guide_60.md\":\"DvOjU6iH\",\"guide_61.md\":\"D_UVP-4q\",\"guide_62.md\":\"BxieeaNO\",\"guide_63.md\":\"BWW5aIaz\",\"guide_64.md\":\"BCmbDGXz\",\"guide_65.md\":\"BjDmo4rh\",\"guide_66.md\":\"CHiZxXNi\",\"guide_67.md\":\"LV1Az369\",\"guide_68.md\":\"0FV73-Yd\",\"guide_7.md\":\"DrPbBXJZ\",\"guide_8.md\":\"XJbW1us8\",\"guide_9.md\":\"yZNnUHnj\",\"index.md\":\"bf4DEKM7\",\"library_1.md\":\"Bro8MpR_\",\"library_10.md\":\"C5E3VKE3\",\"library_11.md\":\"uT2oitmy\",\"library_12.md\":\"PqPRu915\",\"library_13.md\":\"DMA9jDCm\",\"library_14.md\":\"Ca7jRp2O\",\"library_15.md\":\"BkkdSTpM\",\"library_16.md\":\"Bqvdn0_3\",\"library_17.md\":\"CcaCc3Os\",\"library_18.md\":\"BLEXKXgT\",\"library_19.md\":\"YCOxNJUf\",\"library_2.md\":\"DwHdk0NU\",\"library_20.md\":\"Bo3m9rOO\",\"library_21.md\":\"DDBFOA51\",\"library_22.md\":\"CEOirVwB\",\"library_23.md\":\"BerlcUIH\",\"library_24.md\":\"BtvKYo_v\",\"library_25.md\":\"YJjsW_t_\",\"library_26.md\":\"BXcQKvXC\",\"library_27.md\":\"B2BXCf-v\",\"library_28.md\":\"BJxJxyOl\",\"library_29.md\":\"DhbE1XXe\",\"library_3.md\":\"B7yeS1xW\",\"library_30.md\":\"01lvPIZW\",\"library_31.md\":\"DaG0LZ5k\",\"library_32.md\":\"CY_NWWaM\",\"library_33.md\":\"F-rDQqz2\",\"library_34.md\":\"CdgFrq3K\",\"library_35.md\":\"Cf8apfNL\",\"library_36.md\":\"3ef_w6dd\",\"library_37.md\":\"D-NVP6N_\",\"library_38.md\":\"BWZ81QlM\",\"library_39.md\":\"Dj6EHm4F\",\"library_4.md\":\"BqBa09uH\",\"library_40.md\":\"UW0Dk5Dj\",\"library_41.md\":\"CBU2fci3\",\"library_42.md\":\"CtlexrGh\",\"library_43.md\":\"DueJGv-t\",\"library_44.md\":\"BVbT3ElB\",\"library_45.md\":\"BzCJdLR_\",\"library_46.md\":\"Bpo5mTZG\",\"library_47.md\":\"yVmxVWKT\",\"library_48.md\":\"CmM-0GT-\",\"library_49.md\":\"DYXJDTZ8\",\"library_5.md\":\"cJ1MgeD5\",\"library_50.md\":\"Ujn1XVpw\",\"library_51.md\":\"B-ob59St\",\"library_52.md\":\"Bo4ZLud2\",\"library_53.md\":\"-XlOAuac\",\"library_54.md\":\"DpHfJUya\",\"library_55.md\":\"CSldWms9\",\"library_56.md\":\"mIxZEPTO\",\"library_57.md\":\"Js08IVwe\",\"library_58.md\":\"CudoohDT\",\"library_59.md\":\"DqVqWQi3\",\"library_6.md\":\"D8R4z-Bf\",\"library_60.md\":\"CA3UyGgJ\",\"library_61.md\":\"CK97mgIT\",\"library_62.md\":\"4-ddkwln\",\"library_63.md\":\"ChKkduCl\",\"library_64.md\":\"qiQxeWrB\",\"library_65.md\":\"BxpadrGM\",\"library_66.md\":\"DRFy3DR7\",\"library_67.md\":\"CQ_YVesg\",\"library_68.md\":\"DwVtL7wX\",\"library_7.md\":\"BuFnScre\",\"library_8.md\":\"BOxId01p\",\"library_9.md\":\"BBqFfaMg\",\"partners_all.md\":\"DrI_oKr-\",\"partners_curotec.md\":\"C5GS61rR\",\"partners_herodevs.md\":\"BVDxGK-m\",\"partners_index.md\":\"BBcUVcAI\",\"partners_monterail.md\":\"B627I69r\",\"partners_passionatepeople.md\":\"BDAHuxir\",\"partners_redberry.md\":\"B-Sf3MRF\",\"partners_vehikl.md\":\"CjJYaD5l\",\"partners_webreinvent.md\":\"CuHjowhF\",\"quotes_1.md\":\"DPp74516\",\"quotes_10.md\":\"DBy5yQdX\",\"quotes_11.md\":\"wdWW2q-w\",\"quotes_12.md\":\"BoyV9KhA\",\"quotes_13.md\":\"BQzfLnCs\",\"quotes_14.md\":\"CoOOVrS_\",\"quotes_15.md\":\"D5f2jrQk\",\"quotes_16.md\":\"C5XCmr19\",\"quotes_17.md\":\"CGJ9Efvt\",\"quotes_18.md\":\"CZ2XoUV4\",\"quotes_19.md\":\"BubvO7qp\",\"quotes_2.md\":\"DxSYtZMW\",\"quotes_20.md\":\"BMga9fVa\",\"quotes_21.md\":\"2onkwCUB\",\"quotes_22.md\":\"CBp8PzzN\",\"quotes_23.md\":\"BbaThXse\",\"quotes_24.md\":\"BdNkmAXq\",\"quotes_25.md\":\"Dc2Vlnv1\",\"quotes_26.md\":\"C0Xt2czc\",\"quotes_27.md\":\"BXYubb5O\",\"quotes_28.md\":\"1nrEsaBu\",\"quotes_29.md\":\"INyfjVac\",\"quotes_3.md\":\"l0vyD6LI\",\"quotes_30.md\":\"B_utZ1k9\",\"quotes_31.md\":\"DXdfEObj\",\"quotes_32.md\":\"CG4LeQHO\",\"quotes_33.md\":\"WXH3VQD1\",\"quotes_34.md\":\"CZvaHwpB\",\"quotes_35.md\":\"NA-YwQ1l\",\"quotes_36.md\":\"5y_pOKFG\",\"quotes_37.md\":\"eAFXgP3q\",\"quotes_38.md\":\"B5GPVd48\",\"quotes_39.md\":\"DSseBx6I\",\"quotes_4.md\":\"Btwxrc3K\",\"quotes_40.md\":\"OtJjeIrS\",\"quotes_41.md\":\"npmXvlpq\",\"quotes_42.md\":\"BdzIFIQj\",\"quotes_43.md\":\"CxM6YSC2\",\"quotes_44.md\":\"COu2Nnrr\",\"quotes_45.md\":\"XDd1uiF3\",\"quotes_46.md\":\"C_eLaBxj\",\"quotes_47.md\":\"Dp-MEcMU\",\"quotes_48.md\":\"CM2uYfs4\",\"quotes_49.md\":\"D8cipKwz\",\"quotes_5.md\":\"Nb-2iJn3\",\"quotes_50.md\":\"COnnZaEA\",\"quotes_51.md\":\"CftMpfTU\",\"quotes_52.md\":\"CzdLvgPi\",\"quotes_53.md\":\"DPvJi-O7\",\"quotes_54.md\":\"DfdP_vDY\",\"quotes_55.md\":\"Cxj_JbyG\",\"quotes_56.md\":\"Dycbyj9F\",\"quotes_57.md\":\"BeoZFXge\",\"quotes_58.md\":\"CDxF64Ol\",\"quotes_59.md\":\"BrOTScdF\",\"quotes_6.md\":\"8mPBcoqu\",\"quotes_60.md\":\"CyR5st06\",\"quotes_61.md\":\"DTRPfDp5\",\"quotes_62.md\":\"DDu_OeT0\",\"quotes_63.md\":\"B5LJ44pv\",\"quotes_64.md\":\"BKaMZam6\",\"quotes_65.md\":\"YEnhwgNo\",\"quotes_66.md\":\"32dibQIY\",\"quotes_67.md\":\"C63sMeFv\",\"quotes_68.md\":\"B-Vj7X9H\",\"quotes_7.md\":\"CKTwnEUu\",\"quotes_8.md\":\"BDpkZ3_F\",\"quotes_9.md\":\"CPnzktnf\",\"swap_app.md\":\"dX62wfc0\"}");window.__VP_SITE_DATA__=JSON.parse("{\"lang\":\"zh\",\"dir\":\"ltr\",\"title\":\"stanford\",\"description\":\"stanford\",\"base\":\"/\",\"head\":[],\"router\":{\"prefetchLinks\":true},\"appearance\":true,\"themeConfig\":{\"logo\":\"/images/logo.svg\",\"nav\":[{\"text\":\"data\",\"activeMatch\":\"^/drive/\",\"items\":[{\"text\":\"data1\",\"link\":\"/drive/1\"},{\"text\":\"data2\",\"link\":\"/drive/2\"},{\"text\":\"data3\",\"link\":\"/drive/3\"},{\"text\":\"data4\",\"link\":\"/drive/4\"},{\"text\":\"data5\",\"link\":\"/drive/5\"},{\"text\":\"data6\",\"link\":\"/drive/6\"},{\"text\":\"data7\",\"link\":\"/drive/7\"},{\"text\":\"data8\",\"link\":\"/drive/8\"},{\"text\":\"data9\",\"link\":\"/drive/9\"},{\"text\":\"data10\",\"link\":\"/drive/10\"},{\"text\":\"data11\",\"link\":\"/drive/11\"},{\"text\":\"data12\",\"link\":\"/drive/12\"},{\"text\":\"data13\",\"link\":\"/drive/13\"},{\"text\":\"data14\",\"link\":\"/drive/14\"},{\"text\":\"data15\",\"link\":\"/drive/15\"},{\"text\":\"data16\",\"link\":\"/drive/16\"},{\"text\":\"data17\",\"link\":\"/drive/17\"},{\"text\":\"data18\",\"link\":\"/drive/18\"},{\"text\":\"data19\",\"link\":\"/drive/19\"},{\"text\":\"data20\",\"link\":\"/drive/20\"},{\"text\":\"data21\",\"link\":\"/drive/21\"},{\"text\":\"data22\",\"link\":\"/drive/22\"},{\"text\":\"data23\",\"link\":\"/drive/23\"},{\"text\":\"data24\",\"link\":\"/drive/24\"},{\"text\":\"data25\",\"link\":\"/drive/25\"},{\"text\":\"data26\",\"link\":\"/drive/26\"},{\"text\":\"data27\",\"link\":\"/drive/27\"},{\"text\":\"data28\",\"link\":\"/drive/28\"},{\"text\":\"data29\",\"link\":\"/drive/29\"},{\"text\":\"data30\",\"link\":\"/drive/30\"},{\"text\":\"data31\",\"link\":\"/drive/31\"},{\"text\":\"data32\",\"link\":\"/drive/32\"},{\"text\":\"data33\",\"link\":\"/drive/33\"},{\"text\":\"data34\",\"link\":\"/drive/34\"},{\"text\":\"data35\",\"link\":\"/drive/35\"},{\"text\":\"data36\",\"link\":\"/drive/36\"},{\"text\":\"data37\",\"link\":\"/drive/37\"},{\"text\":\"data38\",\"link\":\"/drive/38\"},{\"text\":\"data39\",\"link\":\"/drive/39\"},{\"text\":\"data40\",\"link\":\"/drive/40\"},{\"text\":\"data41\",\"link\":\"/drive/41\"},{\"text\":\"data42\",\"link\":\"/drive/42\"},{\"text\":\"data43\",\"link\":\"/drive/43\"},{\"text\":\"data44\",\"link\":\"/drive/44\"},{\"text\":\"data45\",\"link\":\"/drive/45\"},{\"text\":\"data46\",\"link\":\"/drive/46\"},{\"text\":\"data47\",\"link\":\"/drive/47\"},{\"text\":\"data48\",\"link\":\"/drive/48\"},{\"text\":\"data49\",\"link\":\"/drive/49\"},{\"text\":\"data50\",\"link\":\"/drive/50\"},{\"text\":\"data51\",\"link\":\"/drive/51\"},{\"text\":\"data52\",\"link\":\"/drive/52\"},{\"text\":\"data53\",\"link\":\"/drive/53\"},{\"text\":\"data54\",\"link\":\"/drive/54\"},{\"text\":\"data55\",\"link\":\"/drive/55\"},{\"text\":\"data56\",\"link\":\"/drive/56\"},{\"text\":\"data57\",\"link\":\"/drive/57\"},{\"text\":\"data58\",\"link\":\"/drive/58\"},{\"text\":\"data59\",\"link\":\"/drive/59\"},{\"text\":\"data60\",\"link\":\"/drive/60\"}]},{\"text\":\"grok\",\"activeMatch\":\"^/grok/\",\"items\":[{\"text\":\"grok1\",\"link\":\"/grok/1\"},{\"text\":\"grok2\",\"link\":\"/grok/2\"},{\"text\":\"grok3\",\"link\":\"/grok/3\"},{\"text\":\"grok4\",\"link\":\"/grok/4\"},{\"text\":\"grok5\",\"link\":\"/grok/5\"},{\"text\":\"grok6\",\"link\":\"/grok/6\"},{\"text\":\"grok7\",\"link\":\"/grok/7\"},{\"text\":\"grok8\",\"link\":\"/grok/8\"},{\"text\":\"grok9\",\"link\":\"/grok/9\"},{\"text\":\"grok10\",\"link\":\"/grok/10\"},{\"text\":\"grok11\",\"link\":\"/grok/11\"},{\"text\":\"grok12\",\"link\":\"/grok/12\"},{\"text\":\"grok13\",\"link\":\"/grok/13\"},{\"text\":\"grok14\",\"link\":\"/grok/14\"},{\"text\":\"grok15\",\"link\":\"/grok/15\"},{\"text\":\"grok16\",\"link\":\"/grok/16\"},{\"text\":\"grok17\",\"link\":\"/grok/17\"},{\"text\":\"grok18\",\"link\":\"/grok/18\"},{\"text\":\"grok19\",\"link\":\"/grok/19\"},{\"text\":\"grok20\",\"link\":\"/grok/20\"},{\"text\":\"grok21\",\"link\":\"/grok/21\"},{\"text\":\"grok22\",\"link\":\"/grok/22\"},{\"text\":\"grok23\",\"link\":\"/grok/23\"},{\"text\":\"grok24\",\"link\":\"/grok/24\"},{\"text\":\"grok25\",\"link\":\"/grok/25\"},{\"text\":\"grok26\",\"link\":\"/grok/26\"},{\"text\":\"grok27\",\"link\":\"/grok/27\"},{\"text\":\"grok28\",\"link\":\"/grok/28\"},{\"text\":\"grok29\",\"link\":\"/grok/29\"},{\"text\":\"grok30\",\"link\":\"/grok/30\"},{\"text\":\"grok31\",\"link\":\"/grok/31\"},{\"text\":\"grok32\",\"link\":\"/grok/32\"},{\"text\":\"grok33\",\"link\":\"/grok/33\"},{\"text\":\"grok34\",\"link\":\"/grok/34\"},{\"text\":\"grok35\",\"link\":\"/grok/35\"},{\"text\":\"grok36\",\"link\":\"/grok/36\"},{\"text\":\"grok37\",\"link\":\"/grok/37\"},{\"text\":\"grok38\",\"link\":\"/grok/38\"},{\"text\":\"grok39\",\"link\":\"/grok/39\"},{\"text\":\"grok40\",\"link\":\"/grok/40\"},{\"text\":\"grok41\",\"link\":\"/grok/41\"},{\"text\":\"grok42\",\"link\":\"/grok/42\"},{\"text\":\"grok43\",\"link\":\"/grok/43\"},{\"text\":\"grok44\",\"link\":\"/grok/44\"},{\"text\":\"grok45\",\"link\":\"/grok/45\"},{\"text\":\"grok46\",\"link\":\"/grok/46\"},{\"text\":\"grok47\",\"link\":\"/grok/47\"},{\"text\":\"grok48\",\"link\":\"/grok/48\"},{\"text\":\"grok49\",\"link\":\"/grok/49\"},{\"text\":\"grok50\",\"link\":\"/grok/50\"},{\"text\":\"grok51\",\"link\":\"/grok/51\"},{\"text\":\"grok52\",\"link\":\"/grok/52\"},{\"text\":\"grok53\",\"link\":\"/grok/53\"},{\"text\":\"grok54\",\"link\":\"/grok/54\"},{\"text\":\"grok55\",\"link\":\"/grok/55\"},{\"text\":\"grok56\",\"link\":\"/grok/56\"},{\"text\":\"grok57\",\"link\":\"/grok/57\"},{\"text\":\"grok58\",\"link\":\"/grok/58\"},{\"text\":\"grok59\",\"link\":\"/grok/59\"},{\"text\":\"grok60\",\"link\":\"/grok/60\"},{\"text\":\"grok61\",\"link\":\"/grok/61\"},{\"text\":\"grok62\",\"link\":\"/grok/62\"},{\"text\":\"grok63\",\"link\":\"/grok/63\"},{\"text\":\"grok64\",\"link\":\"/grok/64\"},{\"text\":\"grok65\",\"link\":\"/grok/65\"},{\"text\":\"grok66\",\"link\":\"/grok/66\"},{\"text\":\"grok67\",\"link\":\"/grok/67\"},{\"text\":\"grok68\",\"link\":\"/grok/68\"}]},{\"text\":\"wiki\",\"activeMatch\":\"^/guide/\",\"items\":[{\"text\":\"wiki1\",\"link\":\"/guide/1\"},{\"text\":\"wiki2\",\"link\":\"/guide/2\"},{\"text\":\"wiki3\",\"link\":\"/guide/3\"},{\"text\":\"wiki4\",\"link\":\"/guide/4\"},{\"text\":\"wiki5\",\"link\":\"/guide/5\"},{\"text\":\"wiki6\",\"link\":\"/guide/6\"},{\"text\":\"wiki7\",\"link\":\"/guide/7\"},{\"text\":\"wiki8\",\"link\":\"/guide/8\"},{\"text\":\"wiki9\",\"link\":\"/guide/9\"},{\"text\":\"wiki10\",\"link\":\"/guide/10\"},{\"text\":\"wiki11\",\"link\":\"/guide/11\"},{\"text\":\"wiki12\",\"link\":\"/guide/12\"},{\"text\":\"wiki13\",\"link\":\"/guide/13\"},{\"text\":\"wiki14\",\"link\":\"/guide/14\"},{\"text\":\"wiki15\",\"link\":\"/guide/15\"},{\"text\":\"wiki16\",\"link\":\"/guide/16\"},{\"text\":\"wiki17\",\"link\":\"/guide/17\"},{\"text\":\"wiki18\",\"link\":\"/guide/18\"},{\"text\":\"wiki19\",\"link\":\"/guide/19\"},{\"text\":\"wiki20\",\"link\":\"/guide/20\"},{\"text\":\"wiki21\",\"link\":\"/guide/21\"},{\"text\":\"wiki22\",\"link\":\"/guide/22\"},{\"text\":\"wiki23\",\"link\":\"/guide/23\"},{\"text\":\"wiki24\",\"link\":\"/guide/24\"},{\"text\":\"wiki25\",\"link\":\"/guide/25\"},{\"text\":\"wiki26\",\"link\":\"/guide/26\"},{\"text\":\"wiki27\",\"link\":\"/guide/27\"},{\"text\":\"wiki28\",\"link\":\"/guide/28\"},{\"text\":\"wiki29\",\"link\":\"/guide/29\"},{\"text\":\"wiki30\",\"link\":\"/guide/30\"},{\"text\":\"wiki31\",\"link\":\"/guide/31\"},{\"text\":\"wiki32\",\"link\":\"/guide/32\"},{\"text\":\"wiki33\",\"link\":\"/guide/33\"},{\"text\":\"wiki34\",\"link\":\"/guide/34\"},{\"text\":\"wiki35\",\"link\":\"/guide/35\"},{\"text\":\"wiki36\",\"link\":\"/guide/36\"},{\"text\":\"wiki37\",\"link\":\"/guide/37\"},{\"text\":\"wiki38\",\"link\":\"/guide/38\"},{\"text\":\"wiki39\",\"link\":\"/guide/39\"},{\"text\":\"wiki40\",\"link\":\"/guide/40\"},{\"text\":\"wiki41\",\"link\":\"/guide/41\"},{\"text\":\"wiki42\",\"link\":\"/guide/42\"},{\"text\":\"wiki43\",\"link\":\"/guide/43\"},{\"text\":\"wiki44\",\"link\":\"/guide/44\"},{\"text\":\"wiki45\",\"link\":\"/guide/45\"},{\"text\":\"wiki46\",\"link\":\"/guide/46\"},{\"text\":\"wiki47\",\"link\":\"/guide/47\"},{\"text\":\"wiki48\",\"link\":\"/guide/48\"},{\"text\":\"wiki49\",\"link\":\"/guide/49\"},{\"text\":\"wiki50\",\"link\":\"/guide/50\"},{\"text\":\"wiki51\",\"link\":\"/guide/51\"},{\"text\":\"wiki52\",\"link\":\"/guide/52\"},{\"text\":\"wiki53\",\"link\":\"/guide/53\"},{\"text\":\"wiki54\",\"link\":\"/guide/54\"},{\"text\":\"wiki55\",\"link\":\"/guide/55\"},{\"text\":\"wiki56\",\"link\":\"/guide/56\"},{\"text\":\"wiki57\",\"link\":\"/guide/57\"},{\"text\":\"wiki58\",\"link\":\"/guide/58\"},{\"text\":\"wiki59\",\"link\":\"/guide/59\"},{\"text\":\"wiki60\",\"link\":\"/guide/60\"},{\"text\":\"wiki61\",\"link\":\"/guide/61\"},{\"text\":\"wiki62\",\"link\":\"/guide/62\"},{\"text\":\"wiki63\",\"link\":\"/guide/63\"},{\"text\":\"wiki64\",\"link\":\"/guide/64\"},{\"text\":\"wiki65\",\"link\":\"/guide/65\"},{\"text\":\"wiki66\",\"link\":\"/guide/66\"},{\"text\":\"wiki67\",\"link\":\"/guide/67\"},{\"text\":\"wiki68\",\"link\":\"/guide/68\"}]},{\"text\":\"deep\",\"activeMatch\":\"^/deepseek/\",\"items\":[{\"text\":\"deep1\",\"link\":\"/deepseek/1\"},{\"text\":\"deep2\",\"link\":\"/deepseek/2\"},{\"text\":\"deep3\",\"link\":\"/deepseek/3\"},{\"text\":\"deep4\",\"link\":\"/deepseek/4\"},{\"text\":\"deep5\",\"link\":\"/deepseek/5\"},{\"text\":\"deep6\",\"link\":\"/deepseek/6\"},{\"text\":\"deep7\",\"link\":\"/deepseek/7\"},{\"text\":\"deep8\",\"link\":\"/deepseek/8\"},{\"text\":\"deep9\",\"link\":\"/deepseek/9\"},{\"text\":\"deep10\",\"link\":\"/deepseek/10\"},{\"text\":\"deep11\",\"link\":\"/deepseek/11\"},{\"text\":\"deep12\",\"link\":\"/deepseek/12\"},{\"text\":\"deep13\",\"link\":\"/deepseek/13\"},{\"text\":\"deep14\",\"link\":\"/deepseek/14\"},{\"text\":\"deep15\",\"link\":\"/deepseek/15\"},{\"text\":\"deep16\",\"link\":\"/deepseek/16\"},{\"text\":\"deep17\",\"link\":\"/deepseek/17\"},{\"text\":\"deep18\",\"link\":\"/deepseek/18\"},{\"text\":\"deep19\",\"link\":\"/deepseek/19\"},{\"text\":\"deep20\",\"link\":\"/deepseek/20\"},{\"text\":\"deep21\",\"link\":\"/deepseek/21\"},{\"text\":\"deep22\",\"link\":\"/deepseek/22\"},{\"text\":\"deep23\",\"link\":\"/deepseek/23\"},{\"text\":\"deep24\",\"link\":\"/deepseek/24\"},{\"text\":\"deep25\",\"link\":\"/deepseek/25\"},{\"text\":\"deep26\",\"link\":\"/deepseek/26\"},{\"text\":\"deep27\",\"link\":\"/deepseek/27\"},{\"text\":\"deep28\",\"link\":\"/deepseek/28\"},{\"text\":\"deep29\",\"link\":\"/deepseek/29\"},{\"text\":\"deep30\",\"link\":\"/deepseek/30\"},{\"text\":\"deep31\",\"link\":\"/deepseek/31\"},{\"text\":\"deep32\",\"link\":\"/deepseek/32\"},{\"text\":\"deep33\",\"link\":\"/deepseek/33\"},{\"text\":\"deep34\",\"link\":\"/deepseek/34\"},{\"text\":\"deep35\",\"link\":\"/deepseek/35\"},{\"text\":\"deep36\",\"link\":\"/deepseek/36\"},{\"text\":\"deep37\",\"link\":\"/deepseek/37\"},{\"text\":\"deep38\",\"link\":\"/deepseek/38\"},{\"text\":\"deep39\",\"link\":\"/deepseek/39\"},{\"text\":\"deep40\",\"link\":\"/deepseek/40\"},{\"text\":\"deep41\",\"link\":\"/deepseek/41\"},{\"text\":\"deep42\",\"link\":\"/deepseek/42\"},{\"text\":\"deep43\",\"link\":\"/deepseek/43\"},{\"text\":\"deep44\",\"link\":\"/deepseek/44\"},{\"text\":\"deep45\",\"link\":\"/deepseek/45\"},{\"text\":\"deep46\",\"link\":\"/deepseek/46\"},{\"text\":\"deep47\",\"link\":\"/deepseek/47\"},{\"text\":\"deep48\",\"link\":\"/deepseek/48\"},{\"text\":\"deep49\",\"link\":\"/deepseek/49\"},{\"text\":\"deep50\",\"link\":\"/deepseek/50\"},{\"text\":\"deep51\",\"link\":\"/deepseek/51\"},{\"text\":\"deep52\",\"link\":\"/deepseek/52\"},{\"text\":\"deep53\",\"link\":\"/deepseek/53\"},{\"text\":\"deep54\",\"link\":\"/deepseek/54\"},{\"text\":\"deep55\",\"link\":\"/deepseek/55\"},{\"text\":\"deep56\",\"link\":\"/deepseek/56\"},{\"text\":\"deep57\",\"link\":\"/deepseek/57\"},{\"text\":\"deep58\",\"link\":\"/deepseek/58\"},{\"text\":\"deep59\",\"link\":\"/deepseek/59\"},{\"text\":\"deep60\",\"link\":\"/deepseek/60\"},{\"text\":\"deep61\",\"link\":\"/deepseek/61\"},{\"text\":\"deep62\",\"link\":\"/deepseek/62\"},{\"text\":\"deep63\",\"link\":\"/deepseek/63\"},{\"text\":\"deep64\",\"link\":\"/deepseek/64\"},{\"text\":\"deep65\",\"link\":\"/deepseek/65\"},{\"text\":\"deep66\",\"link\":\"/deepseek/66\"},{\"text\":\"deep67\",\"link\":\"/deepseek/67\"},{\"text\":\"deep68\",\"link\":\"/deepseek/68\"}]},{\"text\":\"quotes\",\"activeMatch\":\"^/quotes/\",\"items\":[{\"text\":\"quotes1\",\"link\":\"/quotes/1\"},{\"text\":\"quotes2\",\"link\":\"/quotes/2\"},{\"text\":\"quotes3\",\"link\":\"/quotes/3\"},{\"text\":\"quotes4\",\"link\":\"/quotes/4\"},{\"text\":\"quotes5\",\"link\":\"/quotes/5\"},{\"text\":\"quotes6\",\"link\":\"/quotes/6\"},{\"text\":\"quotes7\",\"link\":\"/quotes/7\"},{\"text\":\"quotes8\",\"link\":\"/quotes/8\"},{\"text\":\"quotes9\",\"link\":\"/quotes/9\"},{\"text\":\"quotes10\",\"link\":\"/quotes/10\"},{\"text\":\"quotes11\",\"link\":\"/quotes/11\"},{\"text\":\"quotes12\",\"link\":\"/quotes/12\"},{\"text\":\"quotes13\",\"link\":\"/quotes/13\"},{\"text\":\"quotes14\",\"link\":\"/quotes/14\"},{\"text\":\"quotes15\",\"link\":\"/quotes/15\"},{\"text\":\"quotes16\",\"link\":\"/quotes/16\"},{\"text\":\"quotes17\",\"link\":\"/quotes/17\"},{\"text\":\"quotes18\",\"link\":\"/quotes/18\"},{\"text\":\"quotes19\",\"link\":\"/quotes/19\"},{\"text\":\"quotes20\",\"link\":\"/quotes/20\"},{\"text\":\"quotes21\",\"link\":\"/quotes/21\"},{\"text\":\"quotes22\",\"link\":\"/quotes/22\"},{\"text\":\"quotes23\",\"link\":\"/quotes/23\"},{\"text\":\"quotes24\",\"link\":\"/quotes/24\"},{\"text\":\"quotes25\",\"link\":\"/quotes/25\"},{\"text\":\"quotes26\",\"link\":\"/quotes/26\"},{\"text\":\"quotes27\",\"link\":\"/quotes/27\"},{\"text\":\"quotes28\",\"link\":\"/quotes/28\"},{\"text\":\"quotes29\",\"link\":\"/quotes/29\"},{\"text\":\"quotes30\",\"link\":\"/quotes/30\"},{\"text\":\"quotes31\",\"link\":\"/quotes/31\"},{\"text\":\"quotes32\",\"link\":\"/quotes/32\"},{\"text\":\"quotes33\",\"link\":\"/quotes/33\"},{\"text\":\"quotes34\",\"link\":\"/quotes/34\"},{\"text\":\"quotes35\",\"link\":\"/quotes/35\"},{\"text\":\"quotes36\",\"link\":\"/quotes/36\"},{\"text\":\"quotes37\",\"link\":\"/quotes/37\"},{\"text\":\"quotes38\",\"link\":\"/quotes/38\"},{\"text\":\"quotes39\",\"link\":\"/quotes/39\"},{\"text\":\"quotes40\",\"link\":\"/quotes/40\"},{\"text\":\"quotes41\",\"link\":\"/quotes/41\"},{\"text\":\"quotes42\",\"link\":\"/quotes/42\"},{\"text\":\"quotes43\",\"link\":\"/quotes/43\"},{\"text\":\"quotes44\",\"link\":\"/quotes/44\"},{\"text\":\"quotes45\",\"link\":\"/quotes/45\"},{\"text\":\"quotes46\",\"link\":\"/quotes/46\"},{\"text\":\"quotes47\",\"link\":\"/quotes/47\"},{\"text\":\"quotes48\",\"link\":\"/quotes/48\"},{\"text\":\"quotes49\",\"link\":\"/quotes/49\"},{\"text\":\"quotes50\",\"link\":\"/quotes/50\"},{\"text\":\"quotes51\",\"link\":\"/quotes/51\"},{\"text\":\"quotes52\",\"link\":\"/quotes/52\"},{\"text\":\"quotes53\",\"link\":\"/quotes/53\"},{\"text\":\"quotes54\",\"link\":\"/quotes/54\"},{\"text\":\"quotes55\",\"link\":\"/quotes/55\"},{\"text\":\"quotes56\",\"link\":\"/quotes/56\"},{\"text\":\"quotes57\",\"link\":\"/quotes/57\"},{\"text\":\"quotes58\",\"link\":\"/quotes/58\"},{\"text\":\"quotes59\",\"link\":\"/quotes/59\"},{\"text\":\"quotes60\",\"link\":\"/quotes/60\"},{\"text\":\"quotes61\",\"link\":\"/quotes/61\"},{\"text\":\"quotes62\",\"link\":\"/quotes/62\"},{\"text\":\"quotes63\",\"link\":\"/quotes/63\"},{\"text\":\"quotes64\",\"link\":\"/quotes/64\"},{\"text\":\"quotes65\",\"link\":\"/quotes/65\"},{\"text\":\"quotes66\",\"link\":\"/quotes/66\"},{\"text\":\"quotes67\",\"link\":\"/quotes/67\"},{\"text\":\"quotes68\",\"link\":\"/quotes/68\"}]},{\"text\":\"chatai\",\"activeMatch\":\"^/chatai/\",\"items\":[{\"text\":\"chatai1\",\"link\":\"/chatai/1\"},{\"text\":\"chatai2\",\"link\":\"/chatai/2\"},{\"text\":\"chatai3\",\"link\":\"/chatai/3\"},{\"text\":\"chatai4\",\"link\":\"/chatai/4\"},{\"text\":\"chatai5\",\"link\":\"/chatai/5\"},{\"text\":\"chatai6\",\"link\":\"/chatai/6\"},{\"text\":\"chatai7\",\"link\":\"/chatai/7\"},{\"text\":\"chatai8\",\"link\":\"/chatai/8\"},{\"text\":\"chatai9\",\"link\":\"/chatai/9\"},{\"text\":\"chatai10\",\"link\":\"/chatai/10\"},{\"text\":\"chatai11\",\"link\":\"/chatai/11\"},{\"text\":\"chatai12\",\"link\":\"/chatai/12\"},{\"text\":\"chatai13\",\"link\":\"/chatai/13\"},{\"text\":\"chatai14\",\"link\":\"/chatai/14\"},{\"text\":\"chatai15\",\"link\":\"/chatai/15\"},{\"text\":\"chatai16\",\"link\":\"/chatai/16\"},{\"text\":\"chatai17\",\"link\":\"/chatai/17\"},{\"text\":\"chatai18\",\"link\":\"/chatai/18\"},{\"text\":\"chatai19\",\"link\":\"/chatai/19\"},{\"text\":\"chatai20\",\"link\":\"/chatai/20\"},{\"text\":\"chatai21\",\"link\":\"/chatai/21\"},{\"text\":\"chatai22\",\"link\":\"/chatai/22\"},{\"text\":\"chatai23\",\"link\":\"/chatai/23\"},{\"text\":\"chatai24\",\"link\":\"/chatai/24\"},{\"text\":\"chatai25\",\"link\":\"/chatai/25\"},{\"text\":\"chatai26\",\"link\":\"/chatai/26\"},{\"text\":\"chatai27\",\"link\":\"/chatai/27\"},{\"text\":\"chatai28\",\"link\":\"/chatai/28\"},{\"text\":\"chatai29\",\"link\":\"/chatai/29\"},{\"text\":\"chatai30\",\"link\":\"/chatai/30\"},{\"text\":\"chatai31\",\"link\":\"/chatai/31\"},{\"text\":\"chatai32\",\"link\":\"/chatai/32\"},{\"text\":\"chatai33\",\"link\":\"/chatai/33\"},{\"text\":\"chatai34\",\"link\":\"/chatai/34\"},{\"text\":\"chatai35\",\"link\":\"/chatai/35\"},{\"text\":\"chatai36\",\"link\":\"/chatai/36\"},{\"text\":\"chatai37\",\"link\":\"/chatai/37\"},{\"text\":\"chatai38\",\"link\":\"/chatai/38\"},{\"text\":\"chatai39\",\"link\":\"/chatai/39\"},{\"text\":\"chatai40\",\"link\":\"/chatai/40\"},{\"text\":\"chatai41\",\"link\":\"/chatai/41\"},{\"text\":\"chatai42\",\"link\":\"/chatai/42\"},{\"text\":\"chatai43\",\"link\":\"/chatai/43\"},{\"text\":\"chatai44\",\"link\":\"/chatai/44\"},{\"text\":\"chatai45\",\"link\":\"/chatai/45\"},{\"text\":\"chatai46\",\"link\":\"/chatai/46\"},{\"text\":\"chatai47\",\"link\":\"/chatai/47\"},{\"text\":\"chatai48\",\"link\":\"/chatai/48\"},{\"text\":\"chatai49\",\"link\":\"/chatai/49\"},{\"text\":\"chatai50\",\"link\":\"/chatai/50\"},{\"text\":\"chatai51\",\"link\":\"/chatai/51\"},{\"text\":\"chatai52\",\"link\":\"/chatai/52\"},{\"text\":\"chatai53\",\"link\":\"/chatai/53\"},{\"text\":\"chatai54\",\"link\":\"/chatai/54\"},{\"text\":\"chatai55\",\"link\":\"/chatai/55\"},{\"text\":\"chatai56\",\"link\":\"/chatai/56\"},{\"text\":\"chatai57\",\"link\":\"/chatai/57\"},{\"text\":\"chatai58\",\"link\":\"/chatai/58\"},{\"text\":\"chatai59\",\"link\":\"/chatai/59\"},{\"text\":\"chatai60\",\"link\":\"/chatai/60\"},{\"text\":\"chatai61\",\"link\":\"/chatai/61\"},{\"text\":\"chatai62\",\"link\":\"/chatai/62\"},{\"text\":\"chatai63\",\"link\":\"/chatai/63\"},{\"text\":\"chatai64\",\"link\":\"/chatai/64\"},{\"text\":\"chatai65\",\"link\":\"/chatai/65\"},{\"text\":\"chatai66\",\"link\":\"/chatai/66\"},{\"text\":\"chatai67\",\"link\":\"/chatai/67\"},{\"text\":\"chatai68\",\"link\":\"/chatai/68\"}]},{\"text\":\"library\",\"activeMatch\":\"^/library/\",\"items\":[{\"text\":\"library1\",\"link\":\"/library/1\"},{\"text\":\"library2\",\"link\":\"/library/2\"},{\"text\":\"library3\",\"link\":\"/library/3\"},{\"text\":\"library4\",\"link\":\"/library/4\"},{\"text\":\"library5\",\"link\":\"/library/5\"},{\"text\":\"library6\",\"link\":\"/library/6\"},{\"text\":\"library7\",\"link\":\"/library/7\"},{\"text\":\"library8\",\"link\":\"/library/8\"},{\"text\":\"library9\",\"link\":\"/library/9\"},{\"text\":\"library10\",\"link\":\"/library/10\"},{\"text\":\"library11\",\"link\":\"/library/11\"},{\"text\":\"library12\",\"link\":\"/library/12\"},{\"text\":\"library13\",\"link\":\"/library/13\"},{\"text\":\"library14\",\"link\":\"/library/14\"},{\"text\":\"library15\",\"link\":\"/library/15\"},{\"text\":\"library16\",\"link\":\"/library/16\"},{\"text\":\"library17\",\"link\":\"/library/17\"},{\"text\":\"library18\",\"link\":\"/library/18\"},{\"text\":\"library19\",\"link\":\"/library/19\"},{\"text\":\"library20\",\"link\":\"/library/20\"},{\"text\":\"library21\",\"link\":\"/library/21\"},{\"text\":\"library22\",\"link\":\"/library/22\"},{\"text\":\"library23\",\"link\":\"/library/23\"},{\"text\":\"library24\",\"link\":\"/library/24\"},{\"text\":\"library25\",\"link\":\"/library/25\"},{\"text\":\"library26\",\"link\":\"/library/26\"},{\"text\":\"library27\",\"link\":\"/library/27\"},{\"text\":\"library28\",\"link\":\"/library/28\"},{\"text\":\"library29\",\"link\":\"/library/29\"},{\"text\":\"library30\",\"link\":\"/library/30\"},{\"text\":\"library31\",\"link\":\"/library/31\"},{\"text\":\"library32\",\"link\":\"/library/32\"},{\"text\":\"library33\",\"link\":\"/library/33\"},{\"text\":\"library34\",\"link\":\"/library/34\"},{\"text\":\"library35\",\"link\":\"/library/35\"},{\"text\":\"library36\",\"link\":\"/library/36\"},{\"text\":\"library37\",\"link\":\"/library/37\"},{\"text\":\"library38\",\"link\":\"/library/38\"},{\"text\":\"library39\",\"link\":\"/library/39\"},{\"text\":\"library40\",\"link\":\"/library/40\"},{\"text\":\"library41\",\"link\":\"/library/41\"},{\"text\":\"library42\",\"link\":\"/library/42\"},{\"text\":\"library43\",\"link\":\"/library/43\"},{\"text\":\"library44\",\"link\":\"/library/44\"},{\"text\":\"library45\",\"link\":\"/library/45\"},{\"text\":\"library46\",\"link\":\"/library/46\"},{\"text\":\"library47\",\"link\":\"/library/47\"},{\"text\":\"library48\",\"link\":\"/library/48\"},{\"text\":\"library49\",\"link\":\"/library/49\"},{\"text\":\"library50\",\"link\":\"/library/50\"},{\"text\":\"library51\",\"link\":\"/library/51\"},{\"text\":\"library52\",\"link\":\"/library/52\"},{\"text\":\"library53\",\"link\":\"/library/53\"},{\"text\":\"library54\",\"link\":\"/library/54\"},{\"text\":\"library55\",\"link\":\"/library/55\"},{\"text\":\"library56\",\"link\":\"/library/56\"},{\"text\":\"library57\",\"link\":\"/library/57\"},{\"text\":\"library58\",\"link\":\"/library/58\"},{\"text\":\"library59\",\"link\":\"/library/59\"},{\"text\":\"library60\",\"link\":\"/library/60\"},{\"text\":\"library61\",\"link\":\"/library/61\"},{\"text\":\"library62\",\"link\":\"/library/62\"},{\"text\":\"library63\",\"link\":\"/library/63\"},{\"text\":\"library64\",\"link\":\"/library/64\"},{\"text\":\"library65\",\"link\":\"/library/65\"},{\"text\":\"library66\",\"link\":\"/library/66\"},{\"text\":\"library67\",\"link\":\"/library/67\"},{\"text\":\"library68\",\"link\":\"/library/68\"}]},{\"text\":\"ecosystem\",\"activeMatch\":\"^/ecosystem/\",\"items\":[{\"text\":\"website\",\"items\":[{\"text\":\"partners\",\"link\":\"/partners/\"},{\"text\":\"website\",\"link\":\"/ecosystem/themes\"},{\"text\":\"deepseekletters\",\"link\":\"/ecosystem/newsletters\"},{\"text\":\"AI Navigation\",\"link\":\"/ecosystem/navigation\"},{\"text\":\"DeepSeek-V3\",\"link\":\"/ecosystem/DeepSeek\"},{\"text\":\"ChatGPT\",\"link\":\"/ecosystem/ChatGPT\"},{\"text\":\"GPT Prompts\",\"link\":\"/ecosystem/Promptes\"},{\"text\":\"AI Projects\",\"link\":\"/ecosystem/Projects\"},{\"text\":\"474x.com\",\"link\":\"https://www.474x.com\"},{\"text\":\"494x.com\",\"link\":\"https://www.494x.com\"},{\"text\":\"64ii.com\",\"link\":\"https://www.64ii.com\"},{\"text\":\"81oo.com\",\"link\":\"https://www.81oo.com\"}]},{\"text\":\"Library\",\"items\":[{\"text\":\"Vue Router\",\"link\":\"https://e.m44m.com/\"},{\"text\":\"Pinia\",\"link\":\"https://f.m44m.com/\"},{\"text\":\"tool\",\"link\":\"https://www.82ii.com\"}]},{\"text\":\"Vue\",\"items\":[{\"text\":\"Vue Mastery\",\"link\":\"https://g.m44m.com\"},{\"text\":\"Vue School\",\"link\":\"https://h.m44m.com\"}]},{\"text\":\"help\",\"items\":[{\"text\":\"Discord\",\"link\":\"https://i.m44m.com\"},{\"text\":\"GitHub\",\"link\":\"https://github.com/hyaliyun/stanford\"},{\"text\":\"DEV\",\"link\":\"https://www.z2.pw\"}]},{\"text\":\"stanford\",\"items\":[{\"text\":\"blog\",\"link\":\"https://c.m44m.com\"},{\"text\":\"Twitter\",\"link\":\"https://d.m44m.com\"},{\"text\":\"Activity\",\"link\":\"https://e.m44m.com\"},{\"text\":\"CMS\",\"link\":\"https://w.z2.pw\"},{\"text\":\"deepseekmagSheets\",\"link\":\"https://a.z2.pw\"},{\"text\":\"Tailwind\",\"link\":\"https://a.434x.com\"},{\"text\":\"Three.js\",\"link\":\"https://b.434x.com\"},{\"text\":\"youtube\",\"link\":\"https://www.q8q9.com\"}]}]},{\"text\":\"team\",\"link\":\"/about/team\",\"activeMatch\":\"^/about/\"},{\"text\":\"show\",\"activeMatch\":\"^/(guide|style-guide|cookbook|examples)/\",\"items\":[{\"text\":\"donation\",\"link\":\"/drive/donation\"},{\"text\":\"PromptLibrary\",\"link\":\"/drive/PromptLibrary\"},{\"text\":\"prompt\",\"link\":\"/drive/prompt\"},{\"text\":\"Vertex AI\",\"link\":\"/drive/aiprompt\"},{\"text\":\"deepseekStarted\",\"link\":\"/drive/deep\"},{\"text\":\"ChatGPT\",\"link\":\"/drive/gpt\"},{\"text\":\"crypto\",\"link\":\"/drive/team\"},{\"text\":\"partners\",\"link\":\"/partners/\"},{\"text\":\"3kk3.com\",\"link\":\"https://www.3kk3.com\"},{\"text\":\"deepseek\",\"link\":\"https://b.q8q9.com\"},{\"text\":\"deepseekr1\",\"link\":\"https://c.4s5s.com\"},{\"text\":\"deepseekr2\",\"link\":\"https://b.6n7n.com\"},{\"text\":\"deepseekr3\",\"link\":\"https://f.m44m.com\"},{\"text\":\"deepseekr4\",\"link\":\"https://c.q8q9.com\"},{\"text\":\"deepseekr5\",\"link\":\"https://a.l00m.com\"},{\"text\":\"deepseekr6\",\"link\":\"https://g.m44m.com\"}]},{\"text\":\"swap\",\"link\":\"/swap/app\",\"activeMatch\":\"^/swap/\"}],\"sidebar\":{},\"localeLinks\":[{\"link\":\"https://g.m44m.com\",\"text\":\"\",\"repo\":\"https://github.com/hyaliyun/stanford\"}],\"algolia\":{\"indexName\":\"stanford\",\"appId\":\"ML0LEBN7FQ\",\"Key\":\"21cf9df0734770a2448a9da64a700c22\",\"searchParameters\":{\"facetFilters\":[\"version:v3\"]}},\"socialLinks\":[{\"icon\":\"github\",\"link\":\"https://github.com/hyaliyun/stanford/\"}],\"editLink\":{\"repo\":\"hyaliyun/stanford\",\"text\":\"Edit this page on GitHub\"},\"footer\":{\"license\":{\"text\":\"stanford License\",\"link\":\"https://www.m44m.com\"},\"copyright\":\"Copyright  2014-2025 stanford\"}},\"locales\":{},\"scrollOffset\":[\"header\",\".VPLocalNav\"],\"cleanUrls\":false}");</script>
    
  </body>
</html>